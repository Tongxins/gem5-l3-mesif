
/*
 * Copyright (c) 1999-2005 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(L2Cache, "MESIF L2 Cache")
   : CacheMemory * L2cacheMemory,
   int L1ID,
   int banks,
   int num_bits, //?
   int l2_request_latency = 1,
   int l2_response_latency = 1,
   int to_l3_latency = 1
{
//  MessageBuffer unblockToL2Cache, network="From", virtual_network="2", ordered="false", vnet_type="unblock";
  MessageBuffer unblockFromL2Cache, network="To", virtual_network="2", ordered="false", vnet_type="unblock";

  MessageBuffer requestToL2Cache, network="From", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseToL2Cache, network="From", virtual_network="1", ordered="false", vnet_type="response";

  MessageBuffer requestFromL2Cache, network="To", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseFromL2Cache, network="To", virtual_network="1", ordered="false", vnet_type="response";

  // STATES
  state_declaration(State, desc="Cache states", default="L2Cache_State_I") {
    // Base states
    I, AccessPermission:Invalid, desc="a L2 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L2 cache entry Shared";
    SS, AccessPermission:Read_Only, desc="a L2 cache entry Shared and in L1";
    ES, AccessPermission:Read_Only, desc="a L2 cache entry Modifiable, not in L1, but possibly stale";
    EX, AccessPermission:Read_Only, desc="a L2 cache entry Modifiable, possibly modified as L1 has the line";

    // Transient States
    IS, AccessPermission:Busy, desc="L2 idle, issued GETS relay, have not seen response yet";
    IS_I, AccessPermission:Busy, desc="L2 idle, issued GETS relay, have not seen response yet";
    IX, AccessPermission:Busy, desc="L2 idle, issued GETX relay, have not seen response yet";
    SS_I, AccessPermission:Busy, desc="L2 replacing, waiting for InvAck from L1";
    SS_I_INV, AccessPermission:Busy, desc="L2 replacing, waiting for InvAck from L1";
    SS_II, AccessPermission:Busy, desc="L2 invalidated, waiting for InvAck from L1";
    EX_IWB, AccessPermission:Busy, desc="L2 invalidated, waiting for WB Data from L1";
    EX_IWB_SELF, AccessPermission:Busy, desc="L2 replacing, waiting for WB Data from L1";
    EX_IWB_SELF_FWD, AccessPermission:Busy, desc="L2 replacing, waiting for WB Data from L1";
    EX_I_FWD, AccessPermission:Busy, desc="L2 replacing, waiting for WB Data from L1";
    EX_IWB_SELF_NO_PUTX, AccessPermission:Busy, desc="L2 replacing, waiting for WB Data from L1";
    EX_IWB_FWD, AccessPermission:Busy, desc="L2 forwarding, waiting for WB Data from L1";
    EX_IWB_S_FWD, AccessPermission:Busy, desc="L2 forwarding, waiting for WB Data from L1";
    EX_IACK, AccessPermission:Busy, desc="L2 replacing, waiting for WB Ack from L3";
    EX_IACK_NO_PUTX, AccessPermission:Busy, desc="L2 replacing, waiting for WB Ack from L3";
    SS_X, AccessPermission:Read_Only, desc="L1 induced S->M upgrade";
  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // L2 events
    Inv,           desc="Invalidate request from L3 bank";

    L1_GET_INSTR,            desc="a L1I GET INSTR request for a block maped to us";
    L1_GETS,                 desc="a L1D GETS request for a block maped to us";
    L1_GETX,                 desc="a L1D GETX request for a block maped to us";
    L1_UPGRADE,                 desc="a L1D GETX request for a block maped to us";

    L1_PUTX,                 desc="L1 replacing data";

    WB_Data,  desc="data from L1";
    WB_Data_clean,  desc="clean data from L1";
    Ack,      desc="writeback ack";
    Ack_L3,      desc="writeback ack";
    Ack_L3_all,      desc="writeback ack";
    InvAck,      desc="Inv ack";

    // internal generated request ??
    L2_Replacement,  desc="L2 Replacement", format="!r";

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";
    Fwd_GET_INSTR,   desc="GET_INSTR from other processor";

    Data_L2, desc="...";
    Data_L2_all_Acks, desc="...";
    L2_Inv, desc="";
    L2_Ack, desc="";
    L2_Ack_all, desc="";

    Data_L3,       desc="Data for L2";
    Data_L3_All,       desc="Data for L2";
    Data_Exclusive_L3,       desc="Exclusive data for L2";

    WB_Ack,        desc="Ack for replacement from L1";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
  }

  // TBE fields - TBE = Miss Status Handling Register
  structure(TBE, desc="...") {
    Address Address,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty";
    int pendingAcks, default="0", desc="number of pending acks";
    MachineID requestor, desc="requestor";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable L2_TBEs, template_hack="<L2Cache_TBE>";

  int low_bit, default="RubySystem::getBlockSizeBits()";

  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);

  // helper functions
  Entry getCacheEntry(Address addr), return_by_pointer="yes" {
    Entry L2cache_entry := static_cast(Entry, "pointer", L2cacheMemory[addr]);
    DPRINTF(RubySlicc, "Got entry %s\n", L2cache_entry);
    return L2cache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {
    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {
    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Address addr) { // Tell whether readable, dirty
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(tbe.TBEState));
      return L2Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(cache_entry.CacheState));
      return L2Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    //Is not in this cache
    return AccessPermission:NotPresent;
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    //Either in TBE
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
        return tbe.DataBlk;
    }
    //Or just in cache
    return getCacheEntry(addr).DataBlk;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L2Cache_State_to_permission(state));
    }
  }

  int getPendingAcks(TBE tbe) {
    assert(is_valid(tbe));
    return tbe.pendingAcks;
  }

  bool isDirty(Entry cache_entry) {
    assert(is_valid(cache_entry));
    return cache_entry.Dirty;
  }

  Event L1Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }

  Event L3Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }


  out_port(requestIntraChipL2Network_out, RequestMsg, requestFromL2Cache);
  out_port(responseIntraChipL2Network_out, ResponseMsg, responseFromL2Cache);
  out_port(unblockNetwork_out, ResponseMsg, unblockFromL2Cache);

  in_port(requestIntraChipL2Network_in, RequestMsg, requestToL2Cache, rank = 0) {
  if (requestIntraChipL2Network_in.isReady()) {
    peek(requestIntraChipL2Network_in, RequestMsg) {
        assert(in_msg.Destination.isElement(machineID));
        TBE tbe := L2_TBEs[in_msg.Address];
        Entry cache_entry := getCacheEntry(in_msg.Address);
	if (machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache) {
	        DPRINTF(RubySlicc, "Request from L1: Addr: %s State: %s Req: %s Type: %s Dest: %s\n",
                in_msg.Address, getState(tbe, cache_entry, in_msg.Address),
                in_msg.Requestor, in_msg.Type, in_msg.Destination);
	DPRINTF(RubySlicc, "This line in L2: %s\n", cache_entry);
	DPRINTF(RubySlicc, "This line in L2 (TBE): %s\n", tbe);

//       if (is_valid(cache_entry) || in_msg.Type == CoherenceRequestType:PUTX || getState(tbe, cache_entry, in_msg.Address) == State:EX_IACK || getState(tbe, cache_entry, in_msg.Address) == State:EX_IACK_NO_PUTX) { // no need to allocate on PUTX, or when in EX_IACK or some other TODO states!
       if (is_valid(cache_entry)) {
          // The L2 contains the block, so proceeded with handling the request
          trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                in_msg.Requestor, cache_entry),
                  in_msg.Address, cache_entry, tbe);
        } else {
          if (L2cacheMemory.cacheAvail(in_msg.Address) || in_msg.Type == CoherenceRequestType:PUTX) {
          DPRINTF(RubySlicc, "We have space in L2 in state %s,no need to replace for %s\n", getState(tbe, cache_entry, in_msg.Address), in_msg.Address);
            // L2 does't have the line, but we have space for it in the L2
            trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                  in_msg.Requestor, cache_entry),
                    in_msg.Address, cache_entry, tbe);
          } else {
          DPRINTF(RubySlicc, "Np space in L2 in state %s, replacing, caused by %s\n", getState(tbe, cache_entry, in_msg.Address), in_msg.Address);
            // No room in the L2, so we need to make room before handling the request
            Entry L2cache_entry := getCacheEntry(L2cacheMemory.cacheProbe(in_msg.Address));
            trigger(Event:L2_Replacement, L2cacheMemory.cacheProbe(in_msg.Address),
                    L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
          }
        }
	} else if (machineIDToMachineType(in_msg.Requestor) == MachineType:L3Cache) {
			DPRINTF(RubySlicc, "%s\n", in_msg);
		        assert(in_msg.Destination.isElement(machineID));

	DPRINTF(RubySlicc, "Request to L2 from L3: %s\n", in_msg);

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L2_TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceRequestType:INV) {
          trigger(Event:Inv, in_msg.Address, cache_entry, tbe);
        } else {
          error("Invalid L3 request type");
        }

	} else {
		//forwarded L2 request by L3, or response directly from L2
		assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L2Cache);
        	if (in_msg.Type == CoherenceRequestType:GETX || in_msg.Type == CoherenceRequestType:UPGRADE) {
        	// upgrade transforms to GETX due to race
          		trigger(Event:Fwd_GETX, in_msg.Address, cache_entry, tbe);
	        } else if (in_msg.Type == CoherenceRequestType:GETS) {
          		trigger(Event:Fwd_GETS, in_msg.Address, cache_entry, tbe);
	        } else if (in_msg.Type == CoherenceRequestType:GET_INSTR) {
          		trigger(Event:Fwd_GET_INSTR, in_msg.Address, cache_entry, tbe);
		} else if (in_msg.Type == CoherenceRequestType:INV) {
          		trigger(Event:L2_Inv, in_msg.Address, cache_entry, tbe);
		} else {
			error("Invalid request");
		}
    	}
    }}
  }

  // Response IntraChip L2 Network - response msg to this L2 cache - 1st is a port name
  // Basically just triggers actions for arriving responses
  in_port(responseIntraChipL2Network_in, ResponseMsg, responseToL2Cache, rank = 1) {
    if (responseIntraChipL2Network_in.isReady()) { //Seems like the network can block, but why, how?
      peek(responseIntraChipL2Network_in, ResponseMsg) { //block_on??
	DPRINTF(RubySlicc, "%s\n", L2_TBEs[in_msg.Address]);
        assert(in_msg.Destination.isElement(machineID));
	Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L2_TBEs[in_msg.Address];
	DPRINTF(RubySlicc, "Response to in state %s L2: %s", getState(tbe, cache_entry, in_msg.Address), in_msg);
	if (machineIDToMachineType(in_msg.Sender) == MachineType:L3Cache) {
	//************************* L3 BANK *******************************
		//L3 stuff
        	if(in_msg.Type == CoherenceResponseType:DATA_EXCLUSIVE) {
        		trigger(Event:Data_Exclusive_L3, in_msg.Address, cache_entry, tbe);
	        } else if(in_msg.Type == CoherenceResponseType:DATA) {
			
			if ((getPendingAcks(tbe) - in_msg.AckCount == 0)) {
        		trigger(Event:Data_L3_All, in_msg.Address, cache_entry, tbe);
			} else {
        		trigger(Event:Data_L3, in_msg.Address, cache_entry, tbe);
			}
	        } else if (in_msg.Type == CoherenceResponseType:WB_ACK) {
        		trigger(Event:WB_Ack, in_msg.Address, cache_entry, tbe);
	        } else if (in_msg.Type == CoherenceResponseType:ACK) {
			if (getPendingAcks(tbe) - in_msg.AckCount == 0) {
	        		trigger(Event:Ack_L3_all, in_msg.Address, cache_entry, tbe);
			} else {
	        		trigger(Event:Ack_L3, in_msg.Address, cache_entry, tbe);
			}
		} else {
			DPRINTF(RubySlicc, "%s\n", in_msg);
			error("unknown message type from L3");
		}
	} else if (machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache) { //L1 response
	//************************* L1 CACHE *******************************
		DPRINTF(RubySlicc, "Response from L1: %s\n", in_msg);
	        assert(in_msg.Destination.isElement(machineID));
		assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
        	Entry cache_entry := getCacheEntry(in_msg.Address);
	        TBE tbe := L2_TBEs[in_msg.Address];
          	if(in_msg.Type == CoherenceResponseType:DATA) {
			if (in_msg.Dirty) {
				trigger(Event:WB_Data, in_msg.Address, cache_entry, tbe); //L1 changed the data
			} else {
				trigger(Event:WB_Data_clean, in_msg.Address, cache_entry, tbe); //L1 did not change the data
			}
		} else if (in_msg.Type == CoherenceResponseType:ACK) {
			trigger(Event:Ack, in_msg.Address, cache_entry, tbe);
		} else {
			error("unknown message type from L1");
		}
        } else {
	//************************* L2 CACHE *******************************
		assert(machineIDToMachineType(in_msg.Sender) == MachineType:L2Cache);
		DPRINTF(RubySlicc, "Response from L2: %s\n", in_msg);
	        if(in_msg.Type == CoherenceResponseType:DATA) {
		        //pending data
		        if (getPendingAcks(tbe) - in_msg.AckCount != 0) {
			      DPRINTF(RubySlicc, "pending: %s msg: %s\n", getPendingAcks(tbe), in_msg.AckCount);
		              trigger(Event:Data_L2, in_msg.Address, cache_entry, tbe);
			} else {
	                      trigger(Event:Data_L2_all_Acks, in_msg.Address, cache_entry, tbe);
			}
	        } else if (in_msg.Type == CoherenceResponseType:ACK) {
		          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
		            trigger(Event:L2_Ack_all, in_msg.Address, cache_entry, tbe);
		          } else {
		            trigger(Event:L2_Ack, in_msg.Address, cache_entry, tbe);
			}
	        } else {
	          error("Invalid L2 response type");
		}
	}
      }
    }
  }

  // ACTIONS
  action(a_relayRequestToL3, "a", desc="Relay normal request to L3") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := machineID;
        out_msg.DataBlk := in_msg.DataBlk;
        out_msg.Dirty := in_msg.Dirty;
        out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
        DPRINTF(RubySlicc, "Relay to L3 for: address: %s, destination: %s, type %s\n",
                address, out_msg.Destination, in_msg.Type);
        out_msg.MessageSize := in_msg.MessageSize;
        out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  action(ar_relayResponseToL1, "ar", desc="Relay normal request to L3") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := machineID;
        out_msg.DataBlk := in_msg.DataBlk;
        out_msg.Dirty := in_msg.Dirty;
        out_msg.Destination.add(idToMachineID(MachineType:L1Cache, L1ID));
        DPRINTF(RubySlicc, "Relay for: address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := in_msg.MessageSize;
      }
    }
  }

  action(arr_relayDataToRequestor, "arr", desc="send data to requestor") { // ?? This should also be in L3 cache
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(d2_sendDataToL3, "d2", desc="send data to the L3 cache because of M downgrade") { // ?? This should stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(drt_sendDataToRequestor_fromTBE, "drt", desc="send data to requestor") { // ?? Again, move it to L3
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        assert(is_valid(tbe));
	assert(machineIDToMachineType(tbe.requestor) == MachineType:L2Cache);
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.Dirty := tbe.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(tbe.requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
  }

  action(drt_sendDataToImmediateRequestor_fromTBE, "\drt", desc="send data to requestor") { // ?? Again, move it to L3
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.Dirty := tbe.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
}
  }


  action(f_sendDataToL3, "f", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(ft_sendWritebackDataToL3_fromTBE, "ft", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(fi_sendInvAck, "fi", desc="send data to the L3 cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }

  action(fi_sendInvAckFromTBE, "fib", desc="send data to the L3 cache") {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(tbe.requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }


  action(fii_sendInvToL1, "fii", desc="send Inv to the L1 cache") {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceRequestType:INV;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(idToMachineID(MachineType:L1Cache, L1ID));
        out_msg.MessageSize := MessageSizeType:Request_Control;
    }
  }

  action(fa_sendAckToL1, "fa", desc="send Ack to the L1 cache") {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(idToMachineID(MachineType:L1Cache, L1ID));
        out_msg.MessageSize := MessageSizeType:Response_Control;
    }
  }

  action(g_issuePUTX, "g", desc="send data to the L3 cache") {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      if (cache_entry.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(gt_issuePUTXfromTBE, "gt", desc="send data to the L3 cache") {
enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      if (tbe.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(dr_sendResponseToL1, "dr", desc="send shared data to L1 cache") {
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Destination.add(idToMachineID(MachineType:L1Cache, L1ID));
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "To L1 %s %s %s:\n", L1ID, address, out_msg.Destination);
  }}

  action(dre_sendExclusiveDataToL1, "dre", desc="send shared data to L1 cache exlusive") {
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(idToMachineID(MachineType:L1Cache, L1ID));
        out_msg.MessageSize := MessageSizeType:Response_Data; //used to be control?
        DPRINTF(RubySlicc, "%s\n", address);
      }
  }

  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(L2_TBEs);
    L2_TBEs.allocate(address);
    set_tbe(L2_TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
    DPRINTF(RubySlicc, "Allocated TBE for %s %s %s\n", address, tbe, L2_TBEs);
  }

  action(it_TBEsetRequestor, "it", desc="...") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
	    tbe.requestor := in_msg.Requestor;
    }
  }

  action(l_popRequestQueue, "l", desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(o_popResponseQueue, "o", desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    profileMsgDelay(1, responseIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    L2_TBEs.deallocate(address);
    unset_tbe();
  }

  action(u1_writeDataToL2CacheFromResponse, "u1", desc="Write data to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(u2_writeDataToTBEFromResponse, "u2", desc="Write data to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
	      tbe.DataBlk := in_msg.DataBlk;
	      tbe.Dirty := in_msg.Dirty;
    }
  }

  action(u3_writeDataToTBEFromRequest, "u3", desc="Write data to cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      tbe.DataBlk := in_msg.DataBlk;
      tbe.Dirty := in_msg.Dirty;
    }
  }

  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseIntraChipL2Network_in, ResponseMsg) { // Which network ???
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(ff_deallocateL2CacheBlock, "\f", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (L2cacheMemory.isTagPresent(address)) {
      L2cacheMemory.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateL2CacheBlock, "\o", desc="Set L2 cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      DPRINTF(RubySlicc, "L2: Allocating cache entry for address: %s\n", address);
      set_cache_entry(L2cacheMemory.allocate(address, new Entry));
    }
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  GenericRequestType convertToGenericType(CoherenceRequestType type) {
    if(type == CoherenceRequestType:GETS) {
      return GenericRequestType:GETS;
    } else if(type == CoherenceRequestType:GETX) {
      return GenericRequestType:GETX;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return GenericRequestType:GET_INSTR;
    } else if(type == CoherenceRequestType:UPGRADE) {
      return GenericRequestType:UPGRADE;
    } else {
      DPRINTF(RubySlicc, "%s\n", type);
      error("Invalid CoherenceRequestType\n");
    }
  }

  action(uu_profileDataMiss, "\ud", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      L2cacheMemory.profileGenericRequest(convertToGenericType(in_msg.Type),
                                       in_msg.AccessMode, in_msg.Prefetch);
    }
  }

action(zz_stall_incoming_l1_requests, "zz", desc="recycle L1 request queue") {
	stall_and_wait(requestIntraChipL2Network_in, address);
}

  action(t_sendWBAckFromResponse, "t", desc="Send writeback ACK") { //WB_ACK to L1
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=to_l3_latency) {
	assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Sender);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(t_sendWBAckFromRequest, "tt", desc="Send writeback ACK") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=to_l3_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
	assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(writeDataToCacheFromRequest, "mr", desc="Write data from response queue to cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(wbp_sendWBDataFromL1PUTX, "wbp", desc="...") {
	peek(requestIntraChipL2Network_in, RequestMsg) {
		enqueue(responseIntraChipL2Network_out, ResponseMsg) {
			out_msg.Address := in_msg.Address;
			out_msg.DataBlk := in_msg.DataBlk;
        		out_msg.Dirty := in_msg.Dirty;
			out_msg.Type := CoherenceResponseType:DATA;
			out_msg.Sender := machineID;
			out_msg.MessageSize := MessageSizeType:Writeback_Data;
      			out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
		}
	}
  }

  
  action(wbt_sendWBDataToL3FromTBE, "wbt", desc="...") {
	enqueue(responseIntraChipL2Network_out, ResponseMsg) {
		out_msg.Address := address;
		out_msg.Dirty := tbe.Dirty;
		out_msg.DataBlk := tbe.DataBlk;
		out_msg.Type := CoherenceResponseType:DATA;
		out_msg.Sender := machineID;
		out_msg.MessageSize := MessageSizeType:Writeback_Data;
      			out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
	}
  }

action(die, "die", desc="die") {
assert(false);
}

  action(j_sendUnblock, "j", desc="send unblock to the L2 cache") {
    enqueue(unblockNetwork_out, ResponseMsg) {
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:UNBLOCK;
      out_msg.Sender := machineID;
      			out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "%s\n", address);
      
    }
  }

  action(jj_sendExclusiveUnblock, "\j", desc="send unblock to the L2 cache") {
    enqueue(unblockNetwork_out, ResponseMsg) {
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:EXCLUSIVE_UNBLOCK;
      out_msg.Sender := machineID;
      			out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "%s\n", address);

    }
  }

  action(printaddinfo, "fdsadfa", desc="fasdfasdf") {
	peek(responseIntraChipL2Network_in, ResponseMsg) {
	assert(is_valid(tbe));
	DPRINTF(RubySlicc, "%s %s", tbe.DataBlk, in_msg.DataBlk);
	assert(tbe.DataBlk == in_msg.DataBlk);
}
  }

  action(printaddinforeq, "fdsadfaachbozenanebesich", desc="fasdfasdf") {
	if (is_valid(tbe)) {
	peek(requestIntraChipL2Network_in, RequestMsg) {
		DPRINTF(RubySlicc, "%s %s", tbe.DataBlk, in_msg.DataBlk);
		assert(tbe.DataBlk == in_msg.DataBlk);
        }
}
}

  action(gx_issueGETX, "gx", desc="send data to the L3 cache") {
peek(requestIntraChipL2Network_in, RequestMsg) {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      out_msg.Address := in_msg.Address;
      out_msg.Type := CoherenceRequestType:GETX;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToBank(address, MachineType:L3Cache, low_bit, num_bits));
        out_msg.MessageSize := MessageSizeType:Request_Control;
      }
    }
}

  action(dt_sendDataToRequestor_fromTBE, "dt", desc="send data to requestor") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.Dirty := tbe.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }



  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  transition(IS, L2_Replacement) {
    zz_stall_incoming_l1_requests;
  }
  
  // L1 needs data/instruction
  transition(I, {L1_GETS, L1_GET_INSTR}, IS) {
    a_relayRequestToL3;
    oo_allocateL2CacheBlock; // do i need this here, or on hit?
    i_allocateTBE;
    uu_profileDataMiss;
    l_popRequestQueue;
  }
  
  // L1 wants to write to cache block
  transition(I, {L1_GETX}, IX) {
gx_issueGETX;
    oo_allocateL2CacheBlock; // do i need this here, or on hit?
    i_allocateTBE;
    uu_profileDataMiss;
    l_popRequestQueue;
  }

  // Should not happen, we'll see
  transition(I, Inv) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  // Transitions from Shared

  // L1 needs a data we have, but L1 doesn't
  transition(S, {L1_GETS,L1_GET_INSTR}, SS) {
    dr_sendResponseToL1;
l_popRequestQueue;
  }
  
  // L1 needs to write, we have to get the value from L2. Or, we could send it to L1 right away, but that could lead to all sorts of races and we don't want that
  transition({S, SS}, {L1_GETX, L1_UPGRADE}, SS_X) {
    a_relayRequestToL3;
    i_allocateTBE;
    uu_profileDataMiss;
    l_popRequestQueue;
  }

  transition(SS, {L1_GETS, L1_GET_INSTR}) { //L1 silently evicted
    dr_sendResponseToL1;
    l_popRequestQueue;
  }

  transition({IX, SS_X}, {L2_Ack_all, Ack_L3_all}, EX) {
	dre_sendExclusiveDataToL1;
	o_popResponseQueue;
     s_deallocateTBE;
	jj_sendExclusiveUnblock;
	kd_wakeUpDependents;
  }

  transition(SS_X, Ack_L3, SS_X) {
    q_updateAckCount;
    o_popResponseQueue;
  }

  transition(SS_X, {L2_Inv, Inv}, IX) {
fi_sendInvAck;
l_popRequestQueue;
} 

 transition(EX_IWB_S_FWD, {L1_GETS, L1_GETX, L1_GET_INSTR}) {
zz_stall_incoming_l1_requests;
}

  // L1 does not have the value, L3 has clean copy, we can do silent transition
  transition(S, L2_Replacement, I) { // Just L1, L3 can keep the value
    ff_deallocateL2CacheBlock;
  }

  transition(I, L2_Replacement) {
    ff_deallocateL2CacheBlock;
  }
  
  // L1 does have the value, L3 has clean copy, we can do silent transition, but L1 has to be invalidated to comply with inclusivity constraint
  transition(SS, L2_Replacement, SS_I) { // Just L1, L3 can keep the value
    fii_sendInvToL1;
    i_allocateTBE;
    ff_deallocateL2CacheBlock;
  }

  transition(SS_I, Ack, I) { // L1 copy invalidated, we can do silent transition.
    s_deallocateTBE;
    o_popResponseQueue;
kd_wakeUpDependents;
  }

  transition(SS_I, {Inv, L2_Inv}, SS_I_INV) { // Already invalidating
it_TBEsetRequestor;
    l_popRequestQueue;
  }

  transition(ES, {L1_GETS, L1_GET_INSTR}, EX) { //Well, I could add an extra state here, L1 no longer wants to modify this value, but it might still be stale
    dre_sendExclusiveDataToL1;
    l_popRequestQueue;
  }

  transition(ES, L1_GETX, EX) { //Respond to L1, transition to a state with shared M value
    dre_sendExclusiveDataToL1;
    l_popRequestQueue;
  }

  transition(IS, Data_L3_All, SS) { //Got data from L3, relaying to L1
    u1_writeDataToL2CacheFromResponse;
    dr_sendResponseToL1;
    s_deallocateTBE;
    o_popResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, Data_Exclusive_L3, EX) { //Got data from L3, relaying to L1
    u1_writeDataToL2CacheFromResponse;
    dre_sendExclusiveDataToL1;
    o_popResponseQueue;
    s_deallocateTBE;
    jj_sendExclusiveUnblock;
    kd_wakeUpDependents;
  }

  transition(IX, Data_L3_All, EX) { //Got data from L3, relaying to L1, UNBLOCK!!!
    u1_writeDataToL2CacheFromResponse;
    s_deallocateTBE;
    dre_sendExclusiveDataToL1;
    o_popResponseQueue;
    jj_sendExclusiveUnblock;
    kd_wakeUpDependents;
  }

  transition(IX, Data_L2, SS_X) {
u1_writeDataToL2CacheFromResponse;
q_updateAckCount;
o_popResponseQueue;
  }

  transition(IX, Data_L2_all_Acks, EX) { //Got data from all L2s TODO DATA_L2 does not belong here!!!
    u1_writeDataToL2CacheFromResponse;
    s_deallocateTBE;
    dre_sendExclusiveDataToL1;
    o_popResponseQueue;
    jj_sendExclusiveUnblock;
    kd_wakeUpDependents;
  }

  transition(EX, L2_Replacement, EX_IWB_SELF) { // L2 eviction, cannot send data to L3 until L1 writebacks
    i_allocateTBE;
    ff_deallocateL2CacheBlock;
    fii_sendInvToL1;
  }

  transition(EX_IWB_S_FWD, {L2_Replacement}) {
zz_stall_incoming_l1_requests;
}	

  transition(EX, Inv, EX_IWB) { // L2 eviction from L3, cannot send data to L3 until L1 writebacks
    l_popRequestQueue;
    i_allocateTBE;
    ff_deallocateL2CacheBlock;
    fii_sendInvToL1;
  }

  transition(EX_IWB, Inv) { //this is probably bug in router/network
    l_popRequestQueue;
  }

  transition(EX_IWB, L1_PUTX, I) { // L1 wanted eviction *before* we send Inv, relay PUTX with data to L3 and wait for WB_ACK
    u3_writeDataToTBEFromRequest;
    wbp_sendWBDataFromL1PUTX;
    t_sendWBAckFromRequest;
s_deallocateTBE;
kd_wakeUpDependents;
    l_popRequestQueue;
  }

  transition(EX_IWB_FWD, L1_PUTX, I) {
    u3_writeDataToTBEFromRequest;
    drt_sendDataToRequestor_fromTBE;
    t_sendWBAckFromRequest;
s_deallocateTBE;
kd_wakeUpDependents;
    l_popRequestQueue;
  }

  transition(EX_IWB_SELF, L1_PUTX, EX_IACK) {
    u3_writeDataToTBEFromRequest;
    a_relayRequestToL3;
    t_sendWBAckFromRequest;
kd_wakeUpDependents;
    l_popRequestQueue;
  }

  transition(EX_IWB_SELF, {Fwd_GETX}, EX_IWB_SELF_FWD) { // cannot send until writeback arrives!
it_TBEsetRequestor;
l_popRequestQueue;
  }

  transition({EX_IACK, EX_IACK_NO_PUTX}, Ack) { // Lost a race with L1, PUTX issued by L1, but Inv got the L1 anyway and now it sends an Ack
    o_popResponseQueue;
  }

  transition(EX_IWB, {WB_Data, WB_Data_clean}, I) { // Writeback data from L1 after Invalidation, waiting for L3 Inv Ack, L1 does not want a WB_ACK
//    t_sendWBAckFromResponse;
    u2_writeDataToTBEFromResponse;
    wbt_sendWBDataToL3FromTBE;
    o_popResponseQueue;
s_deallocateTBE;
    kd_wakeUpDependents;
  }

  transition(EX_IWB_SELF, {WB_Data, WB_Data_clean}, EX_IACK) {
    u2_writeDataToTBEFromResponse;
//    t_sendWBAckFromResponse;
    gt_issuePUTXfromTBE;
kd_wakeUpDependents;
    o_popResponseQueue;
  }

  transition(EX_IWB_SELF_FWD, {WB_Data, WB_Data_clean}, EX_I_FWD) {
    u2_writeDataToTBEFromResponse;
drt_sendDataToRequestor_fromTBE;
kd_wakeUpDependents;
    o_popResponseQueue;
  }

  transition(EX_IWB_SELF_FWD, L1_PUTX, EX_I_FWD) {
	u3_writeDataToTBEFromRequest;
drt_sendDataToRequestor_fromTBE;
kd_wakeUpDependents;
    l_popRequestQueue;
  }

  transition(EX_I_FWD, Fwd_GETX, I) {
drt_sendDataToImmediateRequestor_fromTBE;
l_popRequestQueue;
  }

  transition(EX_IWB_SELF_NO_PUTX, {WB_Data, WB_Data_clean}, I) {
    u2_writeDataToTBEFromResponse;
    wbt_sendWBDataToL3FromTBE;
    t_sendWBAckFromResponse;
s_deallocateTBE;
kd_wakeUpDependents;
    o_popResponseQueue;
  }

  transition(EX_IWB_SELF_NO_PUTX, L1_PUTX, I) {
    u3_writeDataToTBEFromRequest;
    wbt_sendWBDataToL3FromTBE;
    t_sendWBAckFromRequest;
s_deallocateTBE;
kd_wakeUpDependents;
    l_popRequestQueue;
  }

  transition(EX_IACK, Inv) {
    ft_sendWritebackDataToL3_fromTBE;
    l_popRequestQueue;
  }

  transition({EX_IWB_SELF}, Inv, EX_IWB) { // We cannot invalidate until response with Write Back data from L1 arrives
    l_popRequestQueue;
  }

  transition({EX_IACK, EX_IACK_NO_PUTX}, WB_Ack, I) { // Ack from L3 arrived, can go to starting state
    o_popResponseQueue;
    s_deallocateTBE;
    kd_wakeUpDependents;
  }

  transition(EX, L1_PUTX, ES) { // L1 wants eviction, value can stay in L2
    writeDataToCacheFromRequest;
    t_sendWBAckFromRequest;
    l_popRequestQueue;
  }

  transition(ES, L2_Replacement, EX_IACK) { // L1 does not have the line, only PUTX needed
    g_issuePUTX;
    i_allocateTBE;
    ff_deallocateL2CacheBlock;
  }

  transition(IX, L2_Replacement) { // We have to satisfy previous request first, but it will not serve, since L1 will be invalidated shortly
    zz_stall_incoming_l1_requests;
  }

  transition(ES, Inv, I) { // Cannot be silent, value potentially stale
    f_sendDataToL3;
    ff_deallocateL2CacheBlock;
    l_popRequestQueue;
  }

  transition(S, {Inv, L2_Inv}, I) {
fi_sendInvAck;
l_popRequestQueue;
  }

  transition(EX_IACK, L2_Replacement) {
    zz_stall_incoming_l1_requests;
  }

  transition(I, Ack) { //Race lost, L1 PUTX faster than Inv, Inv arrived anyway and this Ack is a result of it
    o_popResponseQueue;
  }

  transition({EX_IACK, EX_IACK_NO_PUTX}, {L1_GETX, L1_GETS, L1_GET_INSTR}) { // L1 wants previously removed value, we have to stall until L3 writeback is done
    zz_stall_incoming_l1_requests;
  }

  transition({I, EX_IACK, EX_IACK_NO_PUTX}, L1_PUTX) { // Hmm, we've already got WB_IACK, we can drop this. TRIPLE CHECK THIS!
//    printaddinforeq;
    l_popRequestQueue;
  }

  transition(I, {WB_Data, WB_Data_clean}) {
    o_popResponseQueue;
  }

  transition({EX_IACK, EX_IACK_NO_PUTX}, {WB_Data, WB_Data_clean}) { // same as above
//    printaddinfo;
    o_popResponseQueue;
  }

  transition(SS, {Inv, L2_Inv}, SS_I_INV) { 
    fii_sendInvToL1;
    i_allocateTBE;
    ff_deallocateL2CacheBlock;
    it_TBEsetRequestor;
    l_popRequestQueue;
  }

  transition(SS_I_INV, Ack, I) {
    fi_sendInvAckFromTBE;
    s_deallocateTBE;
    o_popResponseQueue;
kd_wakeUpDependents;
  }

  transition(SS_I_INV, {L1_GETX, L1_GETS, L1_GET_INSTR}) {
zz_stall_incoming_l1_requests;
  }

  transition(SS_II, Ack, I) { // L1 copy invalidated, we can go to invalid
    s_deallocateTBE;
    o_popResponseQueue;
  }

  transition(SS_I, {L1_GETX, L1_GETS, L1_GET_INSTR}) {
zz_stall_incoming_l1_requests;
}

  transition(SS_X, L2_Ack) {
	q_updateAckCount;
	o_popResponseQueue;
  }

  transition(EX, L1_UPGRADE, EX) {
    fa_sendAckToL1;
    l_popRequestQueue;
  }

  transition(EX, L1_GETX) { //L1 *had* to send writeback, we have to wait
	zz_stall_incoming_l1_requests;
  }

  transition(ES, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    arr_relayDataToRequestor;
    d2_sendDataToL3;
    l_popRequestQueue;
  }

  transition(EX_IWB_SELF, {Fwd_GETS, Fwd_GET_INSTR}, EX_IWB_S_FWD) {
it_TBEsetRequestor;
zz_stall_incoming_l1_requests;
  }

  transition(ES, Fwd_GETX, I) {
    arr_relayDataToRequestor;
ff_deallocateL2CacheBlock;
    l_popRequestQueue;
  }

  transition(EX, Fwd_GETX, EX_IWB_FWD) {
    i_allocateTBE;
    it_TBEsetRequestor;
    fii_sendInvToL1;
    ff_deallocateL2CacheBlock;
    l_popRequestQueue;
  }

  transition(EX_IWB_FWD, {WB_Data, WB_Data_clean}, I) {
     u2_writeDataToTBEFromResponse;
     drt_sendDataToRequestor_fromTBE;
     s_deallocateTBE;
     o_popResponseQueue;
     kd_wakeUpDependents;
  } 

  transition({EX_IWB}, {L1_GETX}) {
zz_stall_incoming_l1_requests;
  }

  transition({EX_IWB_FWD, EX_IWB_SELF, EX_IWB, EX_IWB_SELF_FWD}, Ack) {
     o_popResponseQueue;
  }

  transition({IX}, {Inv, L2_Inv}) {
fi_sendInvAck;
l_popRequestQueue;
  }

  transition(IS, {L2_Inv, Inv}, IS_I) {
fi_sendInvAck;
 fii_sendInvToL1;
l_popRequestQueue;
  }

  transition(IX, L2_Ack) {
q_updateAckCount;
o_popResponseQueue;
}

  transition(IS_I, Ack) { // all good, no action needed
	o_popResponseQueue;
  }

  transition(IS_I, {Data_L2_all_Acks, Data_L3_All}, I) { //L1 will satisfy the read and drop immediately
ar_relayResponseToL1;
s_deallocateTBE;
o_popResponseQueue;
kd_wakeUpDependents;
  }

  transition(IS_I, L2_Replacement) {
zz_stall_incoming_l1_requests;
  }

  transition(EX_IACK, {Fwd_GET_INSTR, Fwd_GETS}, I) {
    drt_sendDataToImmediateRequestor_fromTBE;
    wbt_sendWBDataToL3FromTBE;
s_deallocateTBE;
    l_popRequestQueue;
  }

  transition(I, WB_Ack) {
o_popResponseQueue;
}

  transition({EX}, {Fwd_GETS, Fwd_GET_INSTR}, EX_IWB_S_FWD) {
    i_allocateTBE;
    it_TBEsetRequestor;
    fii_sendInvToL1;
    l_popRequestQueue;
  }

  transition(EX_IWB_S_FWD, {WB_Data_clean, WB_Data}, S) { //TODO: no need to evict, downgrade is enough
    u1_writeDataToL2CacheFromResponse;
    u2_writeDataToTBEFromResponse;
    drt_sendDataToRequestor_fromTBE;
//send valid data to L3
    d2_sendDataToL3;
    s_deallocateTBE;
    o_popResponseQueue;
kd_wakeUpDependents;
  }


  transition(EX_IWB_S_FWD, {L1_PUTX}, S) {
writeDataToCacheFromRequest;
    u3_writeDataToTBEFromRequest;
t_sendWBAckFromRequest;
    drt_sendDataToRequestor_fromTBE;
    d2_sendDataToL3;
    s_deallocateTBE;
    l_popRequestQueue;
kd_wakeUpDependents;
}

  transition(IS, Data_L2_all_Acks, SS) {
    u1_writeDataToL2CacheFromResponse;
    dr_sendResponseToL1;
    s_deallocateTBE;
    o_popResponseQueue;
    kd_wakeUpDependents;
    j_sendUnblock;
  }

  transition(SS_X, L2_Replacement) {
	zz_stall_incoming_l1_requests;
  }

  transition({IS, IX}, Ack_L3) {
        q_updateAckCount;
	o_popResponseQueue;
  }

  transition(SS, Ack) {
	o_popResponseQueue;
  }

  transition(I, L2_Inv) {
fi_sendInvAck;
l_popRequestQueue;
  }


transition({IX, SS_X}, Data_L3) {
u1_writeDataToL2CacheFromResponse;
q_updateAckCount;
o_popResponseQueue;
}

transition(SS_X, Data_L3_All, EX) { //this should not happen, but i've changed upgrade to getx in L3, no time to do it properly, fix one day
	dre_sendExclusiveDataToL1;
	o_popResponseQueue;
     s_deallocateTBE;
	jj_sendExclusiveUnblock;
	kd_wakeUpDependents;
}


transition(IS, Ack) {
o_popResponseQueue;
}

transition(EX_IWB_S_FWD, Ack) { //we have to wait for WB data
o_popResponseQueue;
}

transition(EX, WB_Data, ES) {
u1_writeDataToL2CacheFromResponse;
o_popResponseQueue;
kd_wakeUpDependents;
}

transition(EX_IACK, Fwd_GETX) {
    drt_sendDataToImmediateRequestor_fromTBE;
//ft_sendWritebackDataToL3_fromTBE;
	l_popRequestQueue;
}

transition({IX, SS_X, S}, Ack){
o_popResponseQueue;
}

transition(EX, Ack) {
o_popResponseQueue;
}

transition(I, L1_UPGRADE, IX) { // this look like inclusivity is broken, but it can only happen in one special case ()
gx_issueGETX;
    oo_allocateL2CacheBlock; // do i need this here, or on hit?
    i_allocateTBE;
    uu_profileDataMiss;
    l_popRequestQueue;
}

}
