
/*
 * Copyright (c) 1999-2005 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(L2Cache, "MESIF Directory L2 Cache Multicore")
   : CacheMemory * L2cacheMemory,
   int id,
   int L2Offset,
   int l3_select_num_bits, //?
   int l2_request_latency = 2,
   int l2_response_latency = 2,
   int to_l3_latency = 1
{
  MessageBuffer unblockToL2Cache, network="From", virtual_network="2", ordered="false", vnet_type="unblock";
  MessageBuffer unblockFromL2Cache, network="To", virtual_network="2", ordered="false", vnet_type="unblock";
//TODO more unblocks

  MessageBuffer requestToL2Cache, network="From", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseToL2Cache, network="From", virtual_network="1", ordered="false", vnet_type="response";

  MessageBuffer requestFromL2Cache, network="To", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseFromL2Cache, network="To", virtual_network="1", ordered="false", vnet_type="response";

  // STATES
  state_declaration(State, desc="Cache states", default="L2Cache_State_I") {
    // Base states
    I, AccessPermission:Invalid, desc="a L2 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L2 cache entry Shared";
    E, AccessPermission:Read_Only, desc="a L2 cache entry Exclusive";
    M, AccessPermission:Read_Write, desc="a L2 cache entry Modified", format="!b";

    // Transient States
    IS, AccessPermission:Busy, desc="L2 idle, issued GETS relay, have not seen response yet";
    IM, AccessPermission:Busy, desc="L2 idle, issued GETX relay, have not seen response yet";
    SM, AccessPermission:Read_Only, desc="L2 idle, issued GETX relay, have not seen response yet";
    IS_I, AccessPermission:Busy, desc="L2 idle, issued GETS relay, saw Inv before data because directory doesn't block on GETS hit";

    M_I, AccessPermission:Busy, desc="L2 replacing, waiting for ACK";
    SINK_WB_ACK, AccessPermission:Busy, desc="This is to sink WB_Acks from L3";

  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // L2 events
    Inv,           desc="Invalidate request from L3 bank";

    L1_GET_INSTR,            desc="a L1I GET INSTR request for a block maped to us";
    L1_GETS,                 desc="a L1D GETS request for a block maped to us";
    L1_GETX,                 desc="a L1D GETX request for a block maped to us";
    L1_UPGRADE,                 desc="a L1D GETX request for a block maped to us";

    L1_PUTX,                 desc="L1 replacing data";
    L1_PUTX_old,             desc="L1 replacing data, but no longer sharer";

    WB_Data,  desc="data from L1";
    WB_Data_clean,  desc="clean data from L1";
    Ack,      desc="writeback ack";
    Ack_all,      desc="writeback ack";

    // internal generated request ??
    L2_Replacement,  desc="L2 Replacement", format="!r";
    L2_Replacement_clean,  desc="L2 Replacement", format="!r";

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";
    Fwd_GET_INSTR,   desc="GET_INSTR from other processor";

    Data,       desc="Data for processor";
    Data_Exclusive,       desc="Data for processor";
    DataS_fromL3,       desc="data for GETS request, need to unblock directory";
    Data_all_Acks,       desc="Data for processor, all acks";

    WB_Ack,        desc="Ack for replacement";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
  }

  // TBE fields - TBE = Miss Status Handling Register
  structure(TBE, desc="...") {
    Address Address,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty";
    bool isPrefetch,       desc="Set if this was caused by a prefetch";
    int pendingAcks, default="0", desc="number of pending acks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable L2_TBEs, template_hack="<L2Cache_TBE>";

  int l3_select_low_bit, default="RubySystem::getBlockSizeBits()";

  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);

  // helper functions
  Entry getCacheEntry(Address addr), return_by_pointer="yes" {
    Entry L2cache_entry := static_cast(Entry, "pointer", L2cacheMemory[addr]);
    DPRINTF(RubySlicc, "Got entry %s\n", L2cache_entry);
    return L2cache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {
//    assert(L2cacheMemory.isTagPresent(addr));  ?????????????
    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {
//    assert((L2cacheMemory.isTagPresent(addr)) == false); ?????????????

    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Address addr) { // Tell whether readable, dirty
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(tbe.TBEState));
      return L2Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(cache_entry.CacheState));
      return L2Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    //Is not in this cache
    return AccessPermission:NotPresent;
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    //Either in TBE
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
        return tbe.DataBlk;
    }
    //Or just in cache
    return getCacheEntry(addr).DataBlk;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L2Cache_State_to_permission(state));
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  bool isDirty(Entry cache_entry) {
    assert(is_valid(cache_entry));
    return cache_entry.Dirty;
  }

  Event L2Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }

  out_port(requestIntraChipL2Network_out, RequestMsg, requestFromL2Cache);
  out_port(responseIntraChipL2Network_out, ResponseMsg, responseFromL2Cache);
  out_port(unblockNetwork_out, ResponseMsg, unblockFromL2Cache);

  in_port(unblockNetwork_in, RequestMsg, unblockToL2Cache, rank = 2) {
	//No unblocks sent by L2 itself, just relay stuff to L3
  	if (unblockNetwork_in.isReady()) {
		peek(unblockNetwork_in, ResponseMsg) {
			enqueue(unblockNetwork_out, ResponseMsg) {
				out_msg.Address := in_msg.Address;
				out_msg.Type := in_msg.Type;
				out_msg.Destination.add(mapAddressToRange(in_msg.Address, MachineType:L3Cache,
                                		        l3_select_low_bit, l3_select_num_bits));
				out_msg.MessageSize := in_msg.MessageSize;
			}
		}
	unblockNetwork_in.dequeue();
	}
  }

  in_port(requestIntraChipL2Network_in, RequestMsg, requestToL2Cache, rank = 0) { //TODO mozna tady ty ranky budou problem
  if (requestIntraChipL2Network_in.isReady()) {
    peek(requestIntraChipL2Network_in, RequestMsg) {
        assert(in_msg.Destination.isElement(machineID));
        TBE tbe := L2_TBEs[in_msg.Address];
        Entry cache_entry := getCacheEntry(in_msg.Address);
	if (machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache) {
	        DPRINTF(RubySlicc, "Request from L1: Addr: %s State: %s Req: %s Type: %s Dest: %s\n",
                in_msg.Address, getState(tbe, cache_entry, in_msg.Address),
                in_msg.Requestor, in_msg.Type, in_msg.Destination);
	DPRINTF(RubySlicc, "This line in L2: %s\n", cache_entry);
	DPRINTF(RubySlicc, "This line in L2 (TBE): %s\n", tbe);

       if (is_valid(cache_entry)) {
          // The L2 contains the block, so proceeded with handling the request
          trigger(L2Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                in_msg.Requestor, cache_entry),
                  in_msg.Address, cache_entry, tbe);
        } else {
          if (L2cacheMemory.cacheAvail(in_msg.Address)) {
            // L2 does't have the line, but we have space for it in the L2
            trigger(L2Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                  in_msg.Requestor, cache_entry),
                    in_msg.Address, cache_entry, tbe);
          } else {
            // No room in the L2, so we need to make room before handling the request
            Entry L2cache_entry := getCacheEntry(L2cacheMemory.cacheProbe(in_msg.Address));
            if (isDirty(L2cache_entry)) {
              trigger(Event:L2_Replacement, L2cacheMemory.cacheProbe(in_msg.Address),
                      L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
            } else {
              trigger(Event:L2_Replacement_clean, L2cacheMemory.cacheProbe(in_msg.Address),
                      L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
            }
          }
        }
	} else {
		assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L3Cache);
		//TODO
assert(false);
	}
    }
    }
  }

  // Response IntraChip L2 Network - response msg to this L2 cache - 1st is a port name
  // Basically just triggers actions for arriving responses
  in_port(responseIntraChipL2Network_in, ResponseMsg, responseToL2Cache, rank = 1) {
    if (responseIntraChipL2Network_in.isReady()) { //Seems like the network can block, but why, how?
      peek(responseIntraChipL2Network_in, ResponseMsg) { //block_on??
	DPRINTF(RubySlicc, "%s\n", L2_TBEs[in_msg.Address]);
	DPRINTF(RubySlicc, "Response to L2: %s", in_msg);
        assert(in_msg.Destination.isElement(machineID));
	if (machineIDToMachineType(in_msg.Sender) == MachineType:L3Cache) {
        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L2_TBEs[in_msg.Address];

        if(in_msg.Type == CoherenceResponseType:DATA_EXCLUSIVE) {
          trigger(Event:Data_Exclusive, in_msg.Address, cache_entry, tbe);
        } else if(in_msg.Type == CoherenceResponseType:DATA) {
          if ((getState(tbe, cache_entry, in_msg.Address) == State:IS ||
               getState(tbe, cache_entry, in_msg.Address) == State:IS_I) &&
              machineIDToMachineType(in_msg.Sender) == MachineType:L3Cache) {
              trigger(Event:DataS_fromL3, in_msg.Address, cache_entry, tbe);
          } else if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Data_all_Acks, in_msg.Address, cache_entry, tbe);
          } else {
            trigger(Event:Data, in_msg.Address, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:ACK) {
          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Ack_all, in_msg.Address, cache_entry, tbe);
          } else {
            trigger(Event:Ack, in_msg.Address, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:WB_ACK) {
          trigger(Event:WB_Ack, in_msg.Address, cache_entry, tbe);
        } else {
          error("Invalid L2 response type");
	  assert(false);
        }
	} else { //L1 response
		assert(false);
	}
      }
    }
  }

  // ACTIONS
  action(a_relayRequestToL3, "a", desc="Relay normal request to L3") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, //What does mapAddressToRange do?
                                                  l3_select_low_bit, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay for: address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  action(d_relayDataToRequestor, "d", desc="send data to requestor") { // ?? This should also be in L3 cache
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(d2_sendDataToL3, "d2", desc="send data to the L3 cache because of M downgrade") { // ?? This should stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, // ?? I guess that direct L3 ID has be here. There's just one L3 for each L2
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(dt_sendDataToRequestor_fromTBE, "dt", desc="send data to requestor") { // ?? Again, move it to L3
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.Dirty := tbe.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(d2t_sendDataToL3_fromTBE, "d2t", desc="send data to the L3 cache") { // This will stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(f_sendDataToL3, "f", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(ft_sendDataToL3_fromTBE, "ft", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(fi_sendInvAck, "fi", desc="send data to the L3 cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }

  action(g_relayPUTX, "g", desc="send data to the L3 cache") {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      if (cache_entry.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(sendResponseToL1, desc="send shared data to L1 cache") { //SHORTHANDS ARE GOOD FOR WHAT?
    peek(responseIntraChipL2Network_in, ResponseMsg) {
//some assert here
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache,
                                                  id, l3_select_num_bits));
        out_msg.MessageSize := in_msg.MessageSize;
        DPRINTF(RubySlicc, "To L1: %s\n", address);
      }
  }}

  action(sendExclusiveDataToL1, desc="send shared data to L1 cache exlusive") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//some assert here
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        DPRINTF(RubySlicc, "%s\n", address);
      }
    }
  }

  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(L2_TBEs);
    assert(is_valid(cache_entry));
    L2_TBEs.allocate(address);
    set_tbe(L2_TBEs[address]);
//    tbe.isPrefetch := false;
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
    DPRINTF(RubySlicc, "Allocated TBE for %s %s %s\n", address, tbe, L2_TBEs);
  }

  action(l_popRequestQueue, "l", desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(o_popIncomingResponseQueue, "o", desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    profileMsgDelay(1, responseIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    L2_TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToL2CacheFromL3, "u3", desc="Write data to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(u_writeDataToL2CacheFromL1, "u1", desc="Write data to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }


  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseIntraChipL2Network_in, ResponseMsg) { // Which network ???
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(ff_deallocateL2CacheBlock, "\f", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (L2cacheMemory.isTagPresent(address)) {
      L2cacheMemory.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateL2CacheBlock, "\o", desc="Set L2 cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      DPRINTF(RubySlicc, "L2: Allocating cache entry for address: %s\n", address);
      set_cache_entry(L2cacheMemory.allocate(address, new Entry));
    }
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  action(uu_profileInstMiss, "\ui", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//Does not compile, no idea why
//        L2cacheMemory.profileMiss(in_msg);
    }
  }

  action(uu_profileDataMiss, "\ud", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//Does not compile, no idea why
//        L2cacheMemory.profileMiss(in_msg);
    }
  }

action(stall_incoming_l1_requests, desc="recycle L1 request queue") {
	stall_and_wait(requestIntraChipL2Network_in, address);
}

  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  // Transitions for Load/Store/Replacement/WriteBack from transient states
  transition({IS, IM, IS_I, M_I, SM, SINK_WB_ACK}, {L1_GETS, L1_GET_INSTR, L1_GETX, L2_Replacement}) {
//TODO some other stall
//    z_stallAndWaitMandatoryQueue;
stall_incoming_l1_requests;
  }

  // Transitions from Idle
  transition(I, L2_Replacement) {
    // Nothing would be sufficient, right?
    ff_deallocateL2CacheBlock;
  }

  transition(I, L1_GETS, IS) {
    a_relayRequestToL3;
    oo_allocateL2CacheBlock;
    i_allocateTBE; // MSHR
    uu_profileDataMiss;
    l_popRequestQueue;
  }

  transition(I, L1_GET_INSTR, IS) {
    oo_allocateL2CacheBlock;
    i_allocateTBE;
    a_relayRequestToL3;
    uu_profileInstMiss;
    l_popRequestQueue;
  }

  transition(I, L1_GETX, IM) {
    oo_allocateL2CacheBlock;
    i_allocateTBE;
    uu_profileDataMiss;
    a_relayRequestToL3;
    l_popRequestQueue;
  }

  transition(I, Inv) {
    fi_sendInvAck;
    l_popRequestQueue; // Mandatory vs. Request?
  }

  // Transitions from Shared
  transition(S, {L1_GETS,L1_GET_INSTR}) {
    sendResponseToL1;
    o_popIncomingResponseQueue;
  }

  transition(S, L1_GETX, SM) {
    i_allocateTBE;
    a_relayRequestToL3;
    uu_profileDataMiss;
    l_popRequestQueue;
  }

  transition(S, L2_Replacement, I) {
    ff_deallocateL2CacheBlock;
  }

  transition(S, Inv, I) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  // Transitions from Exclusive

  transition(E, {L1_GETX, L1_GET_INSTR}) {
    sendResponseToL1;
    o_popIncomingResponseQueue;
  }

//TODO by not specifying this, we imply silent E->M transition
//  transition(E, L1_GETX, M) {
//    hh_store_hit;
//    k_popMandatoryQueue;
//  }

  transition(E, L2_Replacement, M_I) {
    i_allocateTBE;
    g_relayPUTX;   // send data, but hold in case forwarded request
    ff_deallocateL2CacheBlock;
  }

  transition(E, Inv, I) {
    // don't send data
//What about freeing space?
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(E, Fwd_GETX, I) {
    d_relayDataToRequestor;
    l_popRequestQueue;
  }

action(printerr, desc="void")
{
	DPRINTF(RubySlicc, "Got FWD GETS when invalid\n");
        assert(false);
}

  transition(I, Fwd_GETS, I) {
printerr;
  }


  transition(E, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    d_relayDataToRequestor;
    d2_sendDataToL3;
    //todo InformL1
    l_popRequestQueue;
  }

  // Transitions from Modified
  transition(M, {L1_GETX, L1_GET_INSTR}) {
    sendExclusiveDataToL1;
    o_popIncomingResponseQueue;
  }

  transition(M, L2_Replacement, M_I) {
    i_allocateTBE; // Why do we need TBE here?
    g_relayPUTX;   // send data, but hold in case forwarded request
    ff_deallocateL2CacheBlock;
  }

  transition(M_I, WB_Ack, I) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(M, Inv, I) {
    f_sendDataToL3;
    l_popRequestQueue;
  }

  transition(M_I, Inv, SINK_WB_ACK) {
    ft_sendDataToL3_fromTBE;
    l_popRequestQueue;
  }

  transition(M, Fwd_GETX, I) {
    d_relayDataToRequestor;
    l_popRequestQueue;
  }

  transition(M, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    d_relayDataToRequestor;
    d2_sendDataToL3;
    l_popRequestQueue;
  }

  transition(M_I, Fwd_GETX, SINK_WB_ACK) {
    dt_sendDataToRequestor_fromTBE;
    l_popRequestQueue;
  }

  transition(M_I, {Fwd_GETS, Fwd_GET_INSTR}, SINK_WB_ACK) {
    dt_sendDataToRequestor_fromTBE;
    d2t_sendDataToL3_fromTBE;
    l_popRequestQueue;
  }

  // Transitions from IS
  transition({IS, IS_I}, Inv, IS_I) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(IS, Data_all_Acks, S) {
    u_writeDataToL2CacheFromL3; //From what??
//    h_load_hit;
    s_deallocateTBE;
    sendResponseToL1;
    kd_wakeUpDependents;
    o_popIncomingResponseQueue;
  }

  transition(IS_I, Data_all_Acks, I) {
    u_writeDataToL2CacheFromL1; // From what???
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, DataS_fromL3, S) {
    u_writeDataToL2CacheFromL1; // From what???
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS_I, DataS_fromL3, I) { // ????????
    u_writeDataToL2CacheFromL1; // From what??
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // directory is blocked when sending exclusive data
  transition(IS_I, Data_Exclusive, E) {
    u_writeDataToL2CacheFromL1; // From what???
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, Data_Exclusive, E) {
    u_writeDataToL2CacheFromL1; // Frow what??
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // Transitions from IM
  transition({IM, SM}, Inv, IM) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(IM, Data, SM) {
    u_writeDataToL2CacheFromL1;
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(IM, Data_all_Acks, M) {
    u_writeDataToL2CacheFromL1;
//    hh_store_hit;
    s_deallocateTBE;
    sendResponseToL1;
    kd_wakeUpDependents;
    o_popIncomingResponseQueue;
  }

  // transitions from SM
  transition({SM, IM}, Ack) {
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(SM, Ack_all, M) {
//    hh_store_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(SINK_WB_ACK, Inv){
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(SINK_WB_ACK, WB_Ack, I){
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }
}
