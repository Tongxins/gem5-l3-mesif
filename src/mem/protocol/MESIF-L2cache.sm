
/*
 * Copyright (c) 1999-2005 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(L2Cache, "MESIF Directory L2 Cache Multicore")
   : CacheMemory * L2cacheMemory,
   int id,
   int L2Offset,
   int l3_select_num_bits, //?
   int l2_request_latency = 2,
   int l2_response_latency = 2,
   int to_l3_latency = 1
{
  MessageBuffer unblockToL2Cache, network="From", virtual_network="2", ordered="false", vnet_type="unblock";
  MessageBuffer unblockFromL2Cache, network="To", virtual_network="2", ordered="false", vnet_type="unblock";

  MessageBuffer requestToL2Cache, network="From", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseToL2Cache, network="From", virtual_network="1", ordered="false", vnet_type="response";

  MessageBuffer requestFromL2Cache, network="To", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseFromL2Cache, network="To", virtual_network="1", ordered="false", vnet_type="response";

  // STATES
  state_declaration(State, desc="Cache states", default="L2Cache_State_I") {
    // Base states
    I, AccessPermission:Invalid, desc="a L2 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L2 cache entry Shared";
    SS, AccessPermission:Read_Only, desc="a L2 cache entry Shared and in L1";
    ES, AccessPermission:Read_Only, desc="a L2 cache entry Modifiable, not in L1, but possibly stale";
    EX, AccessPermission:Read_Write, desc="a L2 cache entry Modifiable, possibly modified as L1 has the line";

    // Transient States
    IS, AccessPermission:Busy, desc="L2 idle, issued GETS relay, have not seen response yet";
    IX, AccessPermission:Busy, desc="L2 idle, issued GETX relay, have not seen response yet";
    SS_I, AccessPermission:Busy, desc="L2 replacing, waiting for InvAck from L1";
    EX_IWB, AccessPermission:Busy, desc="L2 replacing, waiting for WB Data from L1";
    EX_IACK, AccessPermission:Busy, desc="L2 replacing, waiting for WB Ack from L3";
    SS_X, AccessPermission:Read_Only, desc="L1 induced S->M upgrade";
  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // L2 events
    Inv,           desc="Invalidate request from L3 bank";

    L1_GET_INSTR,            desc="a L1I GET INSTR request for a block maped to us";
    L1_GETS,                 desc="a L1D GETS request for a block maped to us";
    L1_GETX,                 desc="a L1D GETX request for a block maped to us";
    L1_UPGRADE,                 desc="a L1D GETX request for a block maped to us";

    L1_PUTX,                 desc="L1 replacing data";

    WB_Data,  desc="data from L1";
    WB_Data_clean,  desc="clean data from L1";
    Ack,      desc="writeback ack";
    InvAck,      desc="Inv ack";

    // internal generated request ??
    L2_Replacement,  desc="L2 Replacement", format="!r";

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";
    Fwd_GET_INSTR,   desc="GET_INSTR from other processor";

    Data,       desc="Data for L2";
    Data_Exclusive,       desc="Exclusive data for L2";

    WB_Ack,        desc="Ack for replacement from L1";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
  }

  // TBE fields - TBE = Miss Status Handling Register
  structure(TBE, desc="...") {
    Address Address,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty";
    int pendingAcks, default="0", desc="number of pending acks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable L2_TBEs, template_hack="<L2Cache_TBE>";

  int l3_select_low_bit, default="RubySystem::getBlockSizeBits()";

  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);

  // helper functions
  Entry getCacheEntry(Address addr), return_by_pointer="yes" {
    Entry L2cache_entry := static_cast(Entry, "pointer", L2cacheMemory[addr]);
    DPRINTF(RubySlicc, "Got entry %s\n", L2cache_entry);
    return L2cache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {
    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {
    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Address addr) { // Tell whether readable, dirty
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(tbe.TBEState));
      return L2Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(cache_entry.CacheState));
      return L2Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    //Is not in this cache
    return AccessPermission:NotPresent;
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    //Either in TBE
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
        return tbe.DataBlk;
    }
    //Or just in cache
    return getCacheEntry(addr).DataBlk;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L2Cache_State_to_permission(state));
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  bool isDirty(Entry cache_entry) {
    assert(is_valid(cache_entry));
    return cache_entry.Dirty;
  }

  Event L1Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }

  Event L3Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }


  out_port(requestIntraChipL2Network_out, RequestMsg, requestFromL2Cache);
  out_port(responseIntraChipL2Network_out, ResponseMsg, responseFromL2Cache);
  out_port(unblockNetwork_out, ResponseMsg, unblockFromL2Cache);

  in_port(unblockNetwork_in, RequestMsg, unblockToL2Cache, rank = 2) {
	//No unblocks sent by L2 itself, just relay stuff to L3 ? Really, I guess that only L1 blocks, right?
  	if (unblockNetwork_in.isReady()) {
		peek(unblockNetwork_in, ResponseMsg) {
			enqueue(unblockNetwork_out, ResponseMsg) {
				out_msg.Address := in_msg.Address;
				out_msg.Sender := machineID;
				out_msg.Destination.add(mapAddressToRange(in_msg.Address, MachineType:L3Cache,
                                		        l3_select_low_bit, l3_select_num_bits));
				out_msg.Type := in_msg.Type;
				out_msg.MessageSize := in_msg.MessageSize;
			}
		}
		unblockNetwork_in.dequeue();
	}
  }

  in_port(requestIntraChipL2Network_in, RequestMsg, requestToL2Cache, rank = 0) {
  if (requestIntraChipL2Network_in.isReady()) {
    peek(requestIntraChipL2Network_in, RequestMsg) {
        assert(in_msg.Destination.isElement(machineID));
        TBE tbe := L2_TBEs[in_msg.Address];
        Entry cache_entry := getCacheEntry(in_msg.Address);
	if (machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache) {
	        DPRINTF(RubySlicc, "Request from L1: Addr: %s State: %s Req: %s Type: %s Dest: %s\n",
                in_msg.Address, getState(tbe, cache_entry, in_msg.Address),
                in_msg.Requestor, in_msg.Type, in_msg.Destination);
	DPRINTF(RubySlicc, "This line in L2: %s\n", cache_entry);
	DPRINTF(RubySlicc, "This line in L2 (TBE): %s\n", tbe);

       if (is_valid(cache_entry)) {
          // The L2 contains the block, so proceeded with handling the request
          trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                in_msg.Requestor, cache_entry),
                  in_msg.Address, cache_entry, tbe);
        } else {
          if (L2cacheMemory.cacheAvail(in_msg.Address)) {
            // L2 does't have the line, but we have space for it in the L2
            trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                  in_msg.Requestor, cache_entry),
                    in_msg.Address, cache_entry, tbe);
          } else {
            // No room in the L2, so we need to make room before handling the request
            Entry L2cache_entry := getCacheEntry(L2cacheMemory.cacheProbe(in_msg.Address));
//            if (isDirty(L2cache_entry)) {
              trigger(Event:L2_Replacement, L2cacheMemory.cacheProbe(in_msg.Address),
                      L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
//            } else {
//TODO special handling needed? If it is here, then it is in L3, no writebacks needed. Used to be Event:L3_Replacement_clean for some reason.
//              trigger(Event:L2_Replacement_clean, L2cacheMemory.cacheProbe(in_msg.Address),
//                      L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
//            }
          }
        }
	} else {
		assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L3Cache);
		        assert(in_msg.Destination.isElement(machineID));

	DPRINTF(RubySlicc, "Request to L2 from L3: %s\n", in_msg);

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L2_TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceRequestType:INV) {
          trigger(Event:Inv, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETX || in_msg.Type == CoherenceRequestType:UPGRADE) {
assert(false);
          // upgrade transforms to GETX due to race
          trigger(Event:Fwd_GETX, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETS) {
assert(false);
          trigger(Event:Fwd_GETS, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GET_INSTR) {
assert(false);
          trigger(Event:Fwd_GET_INSTR, in_msg.Address, cache_entry, tbe);
        } else {
          error("Invalid forwarded request type");
        }

	}
    }
    }
  }

  // Response IntraChip L2 Network - response msg to this L2 cache - 1st is a port name
  // Basically just triggers actions for arriving responses
  in_port(responseIntraChipL2Network_in, ResponseMsg, responseToL2Cache, rank = 1) {
    if (responseIntraChipL2Network_in.isReady()) { //Seems like the network can block, but why, how?
      peek(responseIntraChipL2Network_in, ResponseMsg) { //block_on??
	DPRINTF(RubySlicc, "%s\n", L2_TBEs[in_msg.Address]);
	DPRINTF(RubySlicc, "Response to L2: %s", in_msg);
        assert(in_msg.Destination.isElement(machineID));
	if (machineIDToMachineType(in_msg.Sender) == MachineType:L3Cache) {
		//L3 stuff
		Entry cache_entry := getCacheEntry(in_msg.Address);
        	TBE tbe := L2_TBEs[in_msg.Address];
        	if(in_msg.Type == CoherenceResponseType:DATA_EXCLUSIVE) {
        		trigger(Event:Data_Exclusive, in_msg.Address, cache_entry, tbe);
	        } else if(in_msg.Type == CoherenceResponseType:DATA) {
        		trigger(Event:Data, in_msg.Address, cache_entry, tbe);
	        } else if (in_msg.Type == CoherenceResponseType:ACK) {
			DPRINTF(RubySlicc, "Got ACK: %s\n", in_msg);
			assert(false); // i don't need ack from L3, do i?
	        } else if (in_msg.Type == CoherenceResponseType:WB_ACK) {
        		trigger(Event:WB_Ack, in_msg.Address, cache_entry, tbe);
		} else {
			error("unknown message type from L3");
		}
	} else { //L1 response
		DPRINTF(RubySlicc, "Response from L1: %s\n", in_msg);
	        assert(in_msg.Destination.isElement(machineID));
		assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
        	Entry cache_entry := getCacheEntry(in_msg.Address);
	        TBE tbe := L2_TBEs[in_msg.Address];
          	if(in_msg.Type == CoherenceResponseType:DATA) {
			if (in_msg.Dirty) {
				trigger(Event:WB_Data, in_msg.Address, cache_entry, tbe); //L1 changed the data
			} else {
				trigger(Event:WB_Data_clean, in_msg.Address, cache_entry, tbe); //L1 did not change the data
			}
		} else if (in_msg.Type == CoherenceResponseType:ACK) {
			trigger(Event:Ack, in_msg.Address, cache_entry, tbe);
		} else {
			error("unknown message type from L1");
		}
        }
      }
    }
  }

  // ACTIONS
  action(a_relayRequestToL3, "a", desc="Relay normal request to L3") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, //What does mapAddressToRange do?
                                                  l3_select_low_bit, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay to L3 for: address: %s, destination: %s, type %s\n",
                address, out_msg.Destination, in_msg.Type);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  // ACTIONS
  action(a_relayResponseToL3, "av", desc="Relay normal request to L3") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, //What does mapAddressToRange do?
                                                  l3_select_low_bit, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay for: address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := in_msg.MessageSize;
      }
    }
  }

  action(a_relayResponseToL1, "ax", desc="Relay normal response to L1") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache, //What does mapAddressToRange do?
                                                  id, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay for: address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := in_msg.MessageSize;
      }
    }
  }

//  action(d_relayDataToRequestor, "d", desc="send data to requestor") { // ?? This should also be in L3 cache
//    peek(requestIntraChipL2Network_in, RequestMsg) {
//      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
//        assert(is_valid(cache_entry));
//        out_msg.Address := address;
//        out_msg.Type := CoherenceResponseType:DATA;
//        out_msg.DataBlk := cache_entry.DataBlk;
//        out_msg.Dirty := cache_entry.Dirty;
//        out_msg.Sender := machineID;
//        out_msg.Destination.add(in_msg.Requestor);
//        out_msg.MessageSize := MessageSizeType:Response_Data;
//      }
//    }
//  }

  action(d2_sendDataToL3, "d2", desc="send data to the L3 cache because of M downgrade") { // ?? This should stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, // ?? I guess that direct L3 ID has be here. There's just one L3 for each L2
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

//  action(dt_sendDataToRequestor_fromTBE, "dt", desc="send data to requestor") { // ?? Again, move it to L3
//    peek(requestIntraChipL2Network_in, RequestMsg) {
//      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
//        assert(is_valid(tbe));
//        out_msg.Address := address;
//        out_msg.Type := CoherenceResponseType:DATA;
//        out_msg.DataBlk := tbe.DataBlk;
//        out_msg.Dirty := tbe.Dirty;
//        out_msg.Sender := machineID;
//        out_msg.Destination.add(in_msg.Requestor);
//        out_msg.MessageSize := MessageSizeType:Response_Data;
//      }
//    }
//  }

  action(d2t_sendDataToL3_fromTBE, "d2t", desc="send data to the L3 cache") { // This will stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(f_sendDataToL3, "f", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data; // response control, response data, writeback data ---------------------------------------------------------1!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1@@@@yeah, i need more actions
    }
  }

  action(ft_sendWritebackDataToL3_fromTBE, "ftdmnc", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(fi_sendInvAck, "fi", desc="send data to the L3 cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
	assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L3Cache);
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }

  action(sendInvToL1, "fii", desc="send Inv to the L1 cache") {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceRequestType:INV;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache,
                                                  id, l3_select_num_bits));
        out_msg.MessageSize := MessageSizeType:Request_Control;
    }
  }

  action(g_issuePUTX, "g", desc="send data to the L3 cache") {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      if (cache_entry.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(g_issuePUTXFromToL1q, "gs", desc="send data to the L3 cache") {
peek(responseIntraChipL2Network_in, ResponseMsg) {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := in_msg.DataBlk;
      out_msg.Dirty := in_msg.Dirty;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      if (in_msg.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
      }
  }
}


  action(g_issuePUTXfromTBE, "g1", desc="send data to the L3 cache") {
enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      if (tbe.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(sendResponseToL1, desc="send shared data to L1 cache") {
//some assert here
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache,
                                                  id, l3_select_num_bits));
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "To L1: %s\n", address);
  }}

  action(sendExclusiveDataToL1, desc="send shared data to L1 cache exlusive") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//some assert here
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data; //used to be control?
        DPRINTF(RubySlicc, "%s\n", address);
      }
    }
  }

  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(L2_TBEs);
    assert(is_valid(cache_entry));
    L2_TBEs.allocate(address);
    set_tbe(L2_TBEs[address]);
//    tbe.isPrefetch := false;
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
    DPRINTF(RubySlicc, "Allocated TBE for %s %s %s\n", address, tbe, L2_TBEs);
  }

  action(popRequestQueue, "l", desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(popResponseQueue, "o", desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    profileMsgDelay(1, responseIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    L2_TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToL2CacheFromRespobse, "u3", desc="Write data to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseIntraChipL2Network_in, ResponseMsg) { // Which network ???
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(ff_deallocateL2CacheBlock, "\f", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (L2cacheMemory.isTagPresent(address)) {
      L2cacheMemory.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateL2CacheBlock, "\o", desc="Set L2 cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      DPRINTF(RubySlicc, "L2: Allocating cache entry for address: %s\n", address);
      set_cache_entry(L2cacheMemory.allocate(address, new Entry));
    }
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  action(uu_profileInstMiss, "\ui", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//Does not compile, no idea why
//        L2cacheMemory.profileMiss(in_msg);
    }
  }

  action(uu_profileDataMiss, "\ud", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//Does not compile, no idea why
//        L2cacheMemory.profileMiss(in_msg);
    }
  }

action(stall_incoming_l1_requests, desc="recycle L1 request queue") {
	stall_and_wait(requestIntraChipL2Network_in, address);
}

action(stall_incoming_l1_responses, desc="recycle L1 request queue") {
	stall_and_wait(responseIntraChipL2Network_in, address);
}

action(downgradeL1, desc="...") {
	assert(false);
}



  action(t_sendWBAckFromResponse, "t", desc="Send writeback ACK") { //WB_ACK to L1
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=to_l3_latency) {
	assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Sender);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(t_sendWBAckFromRequest, "tt", desc="Send writeback ACK") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=to_l3_latency) {
	assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache);
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
	assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(writeDataToCacheFromRequest, "mr", desc="Write data from response queue to cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(sendWBDataFromL1PUTX, desc="...") {
	peek(requestIntraChipL2Network_in, RequestMsg) {
		enqueue(responseIntraChipL2Network_out, ResponseMsg) {
			out_msg.Address := in_msg.Address;
			out_msg.Dirty := in_msg.Dirty;
			out_msg.DataBlk := in_msg.DataBlk;
			out_msg.Type := CoherenceResponseType:DATA;
			out_msg.Sender := machineID;
			out_msg.MessageSize := MessageSizeType:Writeback_Data;
			out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
		}
	}
  }

action(die, "die", desc="die") {
assert(false);
}

  //*****************************************************
  // TRANSITIONS
  //*****************************************************
  
  // L1 needs data/instruction
  transition(I, {L1_GETS, L1_GET_INSTR}, IS) {
    a_relayRequestToL3;
    oo_allocateL2CacheBlock;
    i_allocateTBE; // MSHR
    uu_profileDataMiss;
    popRequestQueue;
  }
  
  // L1 want to write to cache block
  transition(I, L1_GETX, IX) {
    a_relayRequestToL3;
    oo_allocateL2CacheBlock;
    i_allocateTBE;
    uu_profileDataMiss;
    popRequestQueue;
  }

  transition(I, L1_PUTX) {
    die;
  }

  // Should not happen, we'll see
  transition(I, Inv) {
    die;
    fi_sendInvAck;
    popRequestQueue;
  }

  // Transitions from Shared

  // L1 needs a data we have, but L1 doesn't
  transition(S, {L1_GETS,L1_GET_INSTR}, SS) {
    sendResponseToL1;
    popResponseQueue;
  }
  
  // L1 needs to write, we have to get the value from L2. Or, we could send it to L1 right away, but that could lead to all sorts of races and we don't want that
  transition({S, SS}, L1_GETX, SS_X) {
    i_allocateTBE;
    a_relayRequestToL3;
    uu_profileDataMiss;
    popRequestQueue;
  }

  // L1 does not have the value, L3 has clean copy, we can do silent transition
  transition(S, L2_Replacement, I) { // Just L1, L3 can keep the value
    ff_deallocateL2CacheBlock;
  }
  
  // L1 does have the value, L3 has clean copy, we can do silent transition, but L1 has to be invalidated to comply with inclusivity constraint
  transition(SS, L2_Replacement, SS_I) { // Just L1, L3 can keep the value
    // TBE not needed
    fi_sendInvAck;
    ff_deallocateL2CacheBlock;
  }

  transition(ES, L1_GETX, EX) { //Respond to L1, transition to a state with shared M value
    sendResponseToL1;
    popRequestQueue;
  }

  transition(IS, Data, SS) { //Got data from L3, relaying to L1
    u_writeDataToL2CacheFromRespobse;
    sendResponseToL1;
    popResponseQueue;
  }

  transition(IX, Data, EX) { //Got data from L3, relaying to L1
    u_writeDataToL2CacheFromRespobse;
    sendResponseToL1;
    popResponseQueue;
    kd_wakeUpDependents;
  }

  transition(EX, L2_Replacement, EX_IWB) { // L2 eviction, cannot send data to L3 until L1 writebacks
    ff_deallocateL2CacheBlock;
    sendInvToL1;
    //tbe not needed
  }

  transition(EX_IWB, L1_PUTX, EX_IACK) { // L1 wanted eviction *before* we send Inv, relay PUTX with data to L3 and wait for WB_ACK
    a_relayRequestToL3;
    t_sendWBAckFromRequest;
    popRequestQueue;
    kd_wakeUpDependents;
  }

  transition(EX_IACK, Ack, EX_IACK) { // Lost a race with L1, PUTX issued, but Inv got the L1 anyway and now it sends an Ack
    popResponseQueue;
  }

  transition(EX_IWB, {WB_Data, WB_Data_clean}, EX_IACK) { // Writeback data from L1 after Invalidation, waiting for L3 Inv Ack, L1 does not want a WB_ACK
    g_issuePUTXFromToL1q;
    popResponseQueue;
    kd_wakeUpDependents;
  }

  transition(EX_IACK, Inv) { // L3 wants to invalidate while we wait for WB_ACK from it, we can just do nothing and it will deal with this situation, since we've already sent WB_DATA
    popRequestQueue;
  }

  transition(EX_IWB, Inv) { // We cannot invalidate until response with Write Back data from L1 arrives
    stall_incoming_l1_requests;
  }

  transition(EX_IACK, WB_Ack, I) { // Ack from L3 arrived, can go to starting state
    popResponseQueue;
    kd_wakeUpDependents;
  }

  transition(EX, L1_PUTX, ES) { // L1 wants eviction, value can stay in L2
    writeDataToCacheFromRequest;
    t_sendWBAckFromRequest;
    popRequestQueue;
  }

  transition(ES, L2_Replacement, EX_IACK) { // L1 does not have the line, only PUTX needed
    g_issuePUTX;
    ff_deallocateL2CacheBlock;
    //TBE not needed, right?
  }
 
  transition(IX, L2_Replacement) { // We have to satisfy previous request first, but it will not serve, since L1 will be invalidated shortly
    stall_incoming_l1_requests;
  }

  transition(ES, Inv, EX_IACK) { // Cannot be silent, value potentially stale
    f_sendDataToL3;
    ff_deallocateL2CacheBlock;
    popRequestQueue;
  }

  transition(EX_IACK, L1_PUTX) { // We have to stall this request, L1 got evicted too quickly and we are still waiting for L3 WB_ACK
    stall_incoming_l1_requests;
  }
}
