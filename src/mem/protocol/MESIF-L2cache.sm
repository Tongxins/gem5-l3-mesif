
/*
 * Copyright (c) 1999-2005 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(L2Cache, "MESIF Directory L2 Cache Multicore")
   : CacheMemory * L2cacheMemory,
   int id,
   int L2Offset,
   int l3_select_num_bits, //?
   int l2_request_latency = 2,
   int l2_response_latency = 2,
   int to_l3_latency = 1
{
  MessageBuffer unblockToL2Cache, network="From", virtual_network="2", ordered="false", vnet_type="unblock";
  MessageBuffer unblockFromL2Cache, network="To", virtual_network="2", ordered="false", vnet_type="unblock";

  MessageBuffer requestToL2Cache, network="From", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseToL2Cache, network="From", virtual_network="1", ordered="false", vnet_type="response";

  MessageBuffer requestFromL2Cache, network="To", virtual_network="0", ordered="false", vnet_type="request";
  MessageBuffer responseFromL2Cache, network="To", virtual_network="1", ordered="false", vnet_type="response";

  // STATES
  state_declaration(State, desc="Cache states", default="L2Cache_State_I") {
    // Base states
    I, AccessPermission:Invalid, desc="a L2 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L2 cache entry Shared";
    E, AccessPermission:Read_Only, desc="a L2 cache entry Exclusive";
    M, AccessPermission:Read_Write, desc="a L2 cache entry Modified", format="!b";
    M_E, AccessPermission:Read_Write, desc="a L2 cache entry Modified, L1 copy no longer present", format="!b";

    // Transient States
    IS, AccessPermission:Busy, desc="L2 idle, issued GETS relay, have not seen response yet";
    IM, AccessPermission:Busy, desc="L2 idle, issued GETX relay, have not seen response yet";
    SM, AccessPermission:Read_Only, desc="L2 idle, issued GETX relay, have not seen response yet";
    IS_I, AccessPermission:Busy, desc="L2 idle, issued GETS relay, saw Inv before data because directory doesn't block on GETS hit";

    M_I, AccessPermission:Busy, desc="L2 replacing, waiting for first ACK (either L1/L3)";
    S_I, AccessPermission:Busy, desc="L2 replacing, waiting for L1 InvAck";
    M_II, AccessPermission:Busy, desc="L2 replacing, waiting for second ACK (either L1/L3)";
    M_WB, AccessPermission:Busy, desc="L2 replacing from Modified, waiting for WB_Data to send to L3";
    SINK_WB_ACK, AccessPermission:Busy, desc="This is to sink WB_Acks from L3";

  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // L2 events
    Inv,           desc="Invalidate request from L3 bank";

    L1_GET_INSTR,            desc="a L1I GET INSTR request for a block maped to us";
    L1_GETS,                 desc="a L1D GETS request for a block maped to us";
    L1_GETX,                 desc="a L1D GETX request for a block maped to us";
    L1_UPGRADE,                 desc="a L1D GETX request for a block maped to us";

    L1_PUTX,                 desc="L1 replacing data";

    WB_Data,  desc="data from L1";
    WB_Data_clean,  desc="clean data from L1";
    Ack,      desc="writeback ack";
    Ack_all,      desc="writeback ack";

    // internal generated request ??
    L2_Replacement,  desc="L2 Replacement", format="!r";
    L2_Replacement_clean,  desc="L2 Replacement", format="!r";

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";
    Fwd_GET_INSTR,   desc="GET_INSTR from other processor";

    Data,       desc="Data for L2";
    Data_Exclusive,       desc="Exclusive data for L2";

    WB_Ack,        desc="Ack for replacement from L1";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
  }

  // TBE fields - TBE = Miss Status Handling Register
  structure(TBE, desc="...") {
    Address Address,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty";
    int pendingAcks, default="0", desc="number of pending acks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable L2_TBEs, template_hack="<L2Cache_TBE>";

  int l3_select_low_bit, default="RubySystem::getBlockSizeBits()";

  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);

  // helper functions
  Entry getCacheEntry(Address addr), return_by_pointer="yes" {
    Entry L2cache_entry := static_cast(Entry, "pointer", L2cacheMemory[addr]);
    DPRINTF(RubySlicc, "Got entry %s\n", L2cache_entry);
    return L2cache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {
    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {
    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Address addr) { // Tell whether readable, dirty
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(tbe.TBEState));
      return L2Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L2Cache_State_to_permission(cache_entry.CacheState));
      return L2Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    //Is not in this cache
    return AccessPermission:NotPresent;
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    //Either in TBE
    TBE tbe := L2_TBEs[addr];
    if(is_valid(tbe)) {
        return tbe.DataBlk;
    }
    //Or just in cache
    return getCacheEntry(addr).DataBlk;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L2Cache_State_to_permission(state));
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  bool isDirty(Entry cache_entry) {
    assert(is_valid(cache_entry));
    return cache_entry.Dirty;
  }

  Event L1Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }

  Event L3Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    DPRINTF(RubySlicc, "Mapping request from %s to event %s", requestor, type);
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
	return Event:L1_PUTX;
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L1 forwarded request type");
    }
  }


  out_port(requestIntraChipL2Network_out, RequestMsg, requestFromL2Cache);
  out_port(responseIntraChipL2Network_out, ResponseMsg, responseFromL2Cache);
  out_port(unblockNetwork_out, ResponseMsg, unblockFromL2Cache);

  in_port(unblockNetwork_in, RequestMsg, unblockToL2Cache, rank = 2) {
	//No unblocks sent by L2 itself, just relay stuff to L3 ? Really, I guess that only L1 blocks, right?
  	if (unblockNetwork_in.isReady()) {
		peek(unblockNetwork_in, ResponseMsg) {
			enqueue(unblockNetwork_out, ResponseMsg) {
				out_msg := in_msg;//Address;
				out_msg.Sender := machineID;
				out_msg.Destination := (mapAddressToRange(in_msg.Address, MachineType:L3Cache,
                                		        l3_select_low_bit, l3_select_num_bits));
			}
		}
		unblockNetwork_in.dequeue();
	}
  }

  in_port(requestIntraChipL2Network_in, RequestMsg, requestToL2Cache, rank = 0) {
  if (requestIntraChipL2Network_in.isReady()) {
    peek(requestIntraChipL2Network_in, RequestMsg) {
        assert(in_msg.Destination.isElement(machineID));
        TBE tbe := L2_TBEs[in_msg.Address];
        Entry cache_entry := getCacheEntry(in_msg.Address);
	if (machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache) {
	        DPRINTF(RubySlicc, "Request from L1: Addr: %s State: %s Req: %s Type: %s Dest: %s\n",
                in_msg.Address, getState(tbe, cache_entry, in_msg.Address),
                in_msg.Requestor, in_msg.Type, in_msg.Destination);
	DPRINTF(RubySlicc, "This line in L2: %s\n", cache_entry);
	DPRINTF(RubySlicc, "This line in L2 (TBE): %s\n", tbe);

       if (is_valid(cache_entry)) {
          // The L2 contains the block, so proceeded with handling the request
          trigger(L2Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                in_msg.Requestor, cache_entry),
                  in_msg.Address, cache_entry, tbe);
        } else {
          if (L2cacheMemory.cacheAvail(in_msg.Address)) {
            // L2 does't have the line, but we have space for it in the L2
            trigger(L2Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                  in_msg.Requestor, cache_entry),
                    in_msg.Address, cache_entry, tbe);
          } else {
            // No room in the L2, so we need to make room before handling the request
            Entry L2cache_entry := getCacheEntry(L2cacheMemory.cacheProbe(in_msg.Address));
            if (isDirty(L2cache_entry)) {
              trigger(Event:L2_Replacement, L2cacheMemory.cacheProbe(in_msg.Address),
                      L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
            } else {
//TODO special handling needed? If it is here, then it is in L3, no writebacks needed. Used to be Event:L3_Replacement_clean for some reason.
              trigger(Event:L2_Replacement_clean, L2cacheMemory.cacheProbe(in_msg.Address),
                      L2cache_entry, L2_TBEs[L2cacheMemory.cacheProbe(in_msg.Address)]);
            }
          }
        }
	} else {
		assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L3Cache);
		        assert(in_msg.Destination.isElement(machineID));

	DPRINTF(RubySlicc, "Request to L2 from L3: %s\n", in_msg);

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L2_TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceRequestType:INV) {
          trigger(Event:Inv, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETX || in_msg.Type == CoherenceRequestType:UPGRADE) {
assert(false);
          // upgrade transforms to GETX due to race
          trigger(Event:Fwd_GETX, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETS) {
assert(false);
          trigger(Event:Fwd_GETS, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GET_INSTR) {
assert(false);
          trigger(Event:Fwd_GET_INSTR, in_msg.Address, cache_entry, tbe);
        } else {
          error("Invalid forwarded request type");
        }

	}
    }
    }
  }

  // Response IntraChip L2 Network - response msg to this L2 cache - 1st is a port name
  // Basically just triggers actions for arriving responses
  in_port(responseIntraChipL2Network_in, ResponseMsg, responseToL2Cache, rank = 1) {
    if (responseIntraChipL2Network_in.isReady()) { //Seems like the network can block, but why, how?
      peek(responseIntraChipL2Network_in, ResponseMsg) { //block_on??
	DPRINTF(RubySlicc, "%s\n", L2_TBEs[in_msg.Address]);
	DPRINTF(RubySlicc, "Response to L2: %s", in_msg);
        assert(in_msg.Destination.isElement(machineID));
	if (machineIDToMachineType(in_msg.Sender) == MachineType:L3Cache) {
        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L2_TBEs[in_msg.Address];
//TODO something's missing - can have Data in L2 but not here!
        if(in_msg.Type == CoherenceResponseType:DATA_EXCLUSIVE) {
          trigger(Event:Data_Exclusive, in_msg.Address, cache_entry, tbe);
        } else if(in_msg.Type == CoherenceResponseType:DATA) {
          if ((getState(tbe, cache_entry, in_msg.Address) == State:IS ||
               getState(tbe, cache_entry, in_msg.Address) == State:IS_I) &&
              machineIDToMachineType(in_msg.Sender) == MachineType:L3Cache) {
              trigger(Event:DataS_fromL3, in_msg.Address, cache_entry, tbe);
          } else if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Data_all_Acks, in_msg.Address, cache_entry, tbe);
          } else {
            trigger(Event:Data, in_msg.Address, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:ACK) {
	DPRINTF(RubySlicc, "Got ACK: %s\n", in_msg);
          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Ack_all, in_msg.Address, cache_entry, tbe);
          } else {
            trigger(Event:Ack, in_msg.Address, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:WB_ACK) {
          trigger(Event:WB_Ack, in_msg.Address, cache_entry, tbe);
        } else {
          error("Invalid L2 response type");
	  assert(false);
        }
	} else { //L1 response
		DPRINTF(RubySlicc, "Response from L1: %s\n", in_msg);
	        assert(in_msg.Destination.isElement(machineID));
        	Entry cache_entry := getCacheEntry(in_msg.Address);
	        TBE tbe := L2_TBEs[in_msg.Address];
		assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
          	if(in_msg.Type == CoherenceResponseType:DATA) {
			if (in_msg.Dirty) {
				trigger(Event:WB_Data, in_msg.Address, cache_entry, tbe);
			} else {
				trigger(Event:WB_Data_clean, in_msg.Address, cache_entry, tbe);
			}
		} else if (in_msg.Type == CoherenceResponseType:ACK) {
			if ((getPendingAcks(tbe) - in_msg.AckCount) == 0) {
				trigger(Event:Ack_all, in_msg.Address, cache_entry, tbe);
	        		} else {
					trigger(Event:Ack, in_msg.Address, cache_entry, tbe);
				}
		} else {
			error("unknown message type");
		}
        }
      }
    }
  }

  // ACTIONS
  action(a_relayRequestToL3, "a", desc="Relay normal request to L3") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, //What does mapAddressToRange do?
                                                  l3_select_low_bit, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay to L3 for: address: %s, destination: %s, type %s\n",
                address, out_msg.Destination, in_msg.Type);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  // ACTIONS
  action(a_relayResponseToL3, "av", desc="Relay normal request to L3") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, //What does mapAddressToRange do?
                                                  l3_select_low_bit, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay for: address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := in_msg.MessageSize;
      }
    }
  }

  action(a_relayResponseToL1, "ax", desc="Relay normal response to L1") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_request_latency) { // Insert into queue, IntraChip
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache, //What does mapAddressToRange do?
                                                  id, l3_select_num_bits));
        DPRINTF(RubySlicc, "Relay for: address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := in_msg.MessageSize;
      }
    }
  }

//  action(d_relayDataToRequestor, "d", desc="send data to requestor") { // ?? This should also be in L3 cache
//    peek(requestIntraChipL2Network_in, RequestMsg) {
//      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
//        assert(is_valid(cache_entry));
//        out_msg.Address := address;
//        out_msg.Type := CoherenceResponseType:DATA;
//        out_msg.DataBlk := cache_entry.DataBlk;
//        out_msg.Dirty := cache_entry.Dirty;
//        out_msg.Sender := machineID;
//        out_msg.Destination.add(in_msg.Requestor);
//        out_msg.MessageSize := MessageSizeType:Response_Data;
//      }
//    }
//  }

  action(d2_sendDataToL3, "d2", desc="send data to the L3 cache because of M downgrade") { // ?? This should stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache, // ?? I guess that direct L3 ID has be here. There's just one L3 for each L2
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

//  action(dt_sendDataToRequestor_fromTBE, "dt", desc="send data to requestor") { // ?? Again, move it to L3
//    peek(requestIntraChipL2Network_in, RequestMsg) {
//      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
//        assert(is_valid(tbe));
//        out_msg.Address := address;
//        out_msg.Type := CoherenceResponseType:DATA;
//        out_msg.DataBlk := tbe.DataBlk;
//        out_msg.Dirty := tbe.Dirty;
//        out_msg.Sender := machineID;
//        out_msg.Destination.add(in_msg.Requestor);
//        out_msg.MessageSize := MessageSizeType:Response_Data;
//      }
//    }
//  }

  action(d2t_sendDataToL3_fromTBE, "d2t", desc="send data to the L3 cache") { // This will stay
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(f_sendDataToL3, "f", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Response_Data; // response control, response data, writeback data ---------------------------------------------------------1!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!11
    }
  }

  action(ft_sendWritebackDataToL3_fromTBE, "ftdmnc", desc="send data to the L3 cache") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(fi_sendInvAck, "fi", desc="send data to the L3 cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
	assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L3Cache);
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }

  action(sendInvToL1, "fii", desc="send Inv to the L1 cache") {
      enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceRequestType:INV;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache,
                                                  id, l3_select_num_bits));
        out_msg.MessageSize := MessageSizeType:Request_Control;
    }
  }

  action(g_issuePUTX, "g", desc="send data to the L3 cache") {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      if (cache_entry.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(g_issuePUTXFromToL1q, "gs", desc="send data to the L3 cache") {
peek(responseIntraChipL2Network_in, ResponseMsg) {
    enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := in_msg.DataBlk;
      out_msg.Dirty := in_msg.Dirty;
      out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
        out_msg.MessageSize := in_msg.Request;
      }
  }
}


  action(g_issuePUTXfromTBE, "g1", desc="send data to the L3 cache") {
enqueue(requestIntraChipL2Network_out, RequestMsg, latency=l2_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L3Cache,
                                                  l3_select_low_bit, l3_select_num_bits));
      if (tbe.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(sendResponseToL1, desc="send shared data to L1 cache") {
//some assert here
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L1Cache,
                                                  id, l3_select_num_bits));
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "To L1: %s\n", address);
  }}

  action(sendExclusiveDataToL1, desc="send shared data to L1 cache exlusive") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//some assert here
      enqueue(responseIntraChipL2Network_out, ResponseMsg) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Data; //used to be control?
        DPRINTF(RubySlicc, "%s\n", address);
      }
    }
  }

  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(L2_TBEs);
    assert(is_valid(cache_entry));
    L2_TBEs.allocate(address);
    set_tbe(L2_TBEs[address]);
//    tbe.isPrefetch := false;
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
    DPRINTF(RubySlicc, "Allocated TBE for %s %s %s\n", address, tbe, L2_TBEs);
  }

  action(popIncomingRequestQueue, "l", desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(popIncomingResponseQueue, "o", desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    profileMsgDelay(1, responseIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    L2_TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToL2CacheFromRespobse, "u3", desc="Write data to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseIntraChipL2Network_in, ResponseMsg) { // Which network ???
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(ff_deallocateL2CacheBlock, "\f", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (L2cacheMemory.isTagPresent(address)) {
      L2cacheMemory.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateL2CacheBlock, "\o", desc="Set L2 cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      DPRINTF(RubySlicc, "L2: Allocating cache entry for address: %s\n", address);
      set_cache_entry(L2cacheMemory.allocate(address, new Entry));
    }
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  action(uu_profileInstMiss, "\ui", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//Does not compile, no idea why
//        L2cacheMemory.profileMiss(in_msg);
    }
  }

  action(uu_profileDataMiss, "\ud", desc="Profile the demand miss") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
//Does not compile, no idea why
//        L2cacheMemory.profileMiss(in_msg);
    }
  }

action(stall_incoming_l1_requests, desc="recycle L1 request queue") {
	stall_and_wait(requestIntraChipL2Network_in, address);
}

action(stall_incoming_l1_responses, desc="recycle L1 request queue") {
	stall_and_wait(responseIntraChipL2Network_in, address);
}



  action(t_sendWBAckFromResponse, "t", desc="Send writeback ACK") { //WB_ACK to L1
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=to_l3_latency) {
	assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Sender);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(t_sendWBAckFromRequest, "tt", desc="Send writeback ACK") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency=to_l3_latency) {
	assert(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache);
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
	assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(writeDataToCacheFromRequest, "mr", desc="Write data from response queue to cache") {
    peek(requestIntraChipL2Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

action(die, "die", desc="die") {
assert(false);
}

  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  // Transitions for Load/Store/Replacement/WriteBack from transient states //TODO recently added M_II and L1_PUTX event, state, should be there I guess, but iff waiting for ack from L3, maybe an extra state will be needed after all.
  transition({IS, IM, IS_I, M_I, SM, SINK_WB_ACK}, {L1_GETS, L1_GET_INSTR, L1_GETX, L2_Replacement}) {
stall_incoming_l1_requests;
  }

  // Transitions from I (Idle)
//TODO all transient states, added M_WB, although is should not be there imo
// L1_PUTX only makes sense in one of the base states I guess
  transition(SI, L1_PUTX) { // Only clean states!
//stall_incoming_l1_requests; ???
    t_sendWBAckFromReq;
    popRequestQueue;
  }

  transition(I, {L1_GETS, L1_GET_INSTR}, IS) {
    a_relayRequestToL3;
    oo_allocateL2CacheBlock;
    i_allocateTBE; // MSHR
    uu_profileDataMiss;
    l_popRequestQueue;
  }

  transition(I, L1_GETX, IM) {
    oo_allocateL2CacheBlock;
    i_allocateTBE;
    uu_profileDataMiss;
    a_relayRequestToL3;
    l_popRequestQueue;
  }

  transition({I, M_WB}, Inv) { //Inv only comes from L3
    fi_sendInvAck;
    l_popRequestQueue; // Mandatory vs. Request?
  }

//TODO does not make sense toto je uplne spatne proboha ziveho
//	transition(M_I, {WB_Data, WB_Data_clean}) {
//		a_relayResponseToL3;
//		o_popIncomingResponseQueue;
//	}

kokot
transition(M_WB, L1_GETX, IM) {
	//implies that WB from L1 was sent, right? Or L1 did not have the value. Not really, we could have lost a race before or L1 silenty transitioned?
    oo_allocateL2CacheBlock;
//    i_allocateTBE; //i don't think TBE is needed here TODO
    uu_profileDataMiss;
    a_relayRequestToL3;
    l_popRequestQueue;
}

transition(M_WB, L1_PUTX, M_I) {
//L2 has lost a race with L1, sort of at least. 
oo_allocateL2CacheBlock;
t_sendWBAckFromReq;
a_relayRequestToL3;
l_popRequestQueue;
}

//TODO taky ne. protoze, nejaky M_I, nebo S_I stav leda tak.
//	transition(I, WB_Ack) {		
//		o_popIncomingResponseQueue;
//	}

  // Transitions from Shared
  transition(S, {L1_GETS,L1_GET_INSTR}) {
    sendResponseToL1;
    o_popIncomingResponseQueue;
  }

  transition(S, L1_GETX, SM) {
    i_allocateTBE;
    a_relayRequestToL3;
    uu_profileDataMiss;
    l_popRequestQueue;
  }

  transition(S, L2_Replacement, S_I) { // Just L1, L3 can keep the value
    sendInvToL1;
    i_allocateTBE;
    ff_deallocateL2CacheBlock;
  }
 

   //TODO responses from S_I

//   transition(S_I, Ack, I) {
//	s_deallocateTBE;
//	o_popIncomingResponseQueue;
//   }

   transition(S_I, InvAck, I) {
	s_deallocateTBE;
	o_popIncomingResponseQueue;
   }

  transition(M, L1_PUTX, M_E) {
//	a_relayRequestToL3; // TODO instead of sending this to L3, change the state
	writeDataToCacheFromRequest;
	t_sendWBAckFromReq;
	l_popRequestQueue;
  }

  transition(S, Inv, I) { //has to go to L1 as well, no need to wait for WB_ACK, there will be none
    sendInvToL1;
    fi_sendInvAck;
    l_popRequestQueue;
  }

  // Transitions from Exclusive

  transition(E_S, {L1_GETX, L1_GET_INSTR}, E_M) {
    sendResponseToL1;
    o_popIncomingResponseQueue;
  }

//TODO by not specifying this, we imply silent E->M transition
//  transition(E, L1_GETX, M) {
//    hh_store_hit;
//    k_popMandatoryQueue;
//  }

  //going to S_I, used to be M_I, but we only need ack from L3
//do i really need to send data to L3? I guess not.
  transition(E, L2_Replacement, S_I) {
    //silent L2 eviction for L3, but L1 has to be evicted
    sendInvToL1;
    i_allocateTBE; // not sure at all about this, would be ok to drop totally i guess
    g_issuePUTX;   // send data, but hold in case forwarded request
    ff_deallocateL2CacheBlock;
  }

  transition(E_S, Inv, I) {
    // don't send data
//What about freeing space?
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(E_M, Inv, M_I) { //TODO the rigth state? We need data from L1, since E->M transition is silent. Maybe change.
	//neco
  }

  transition(E, Fwd_GETX, I) { //this is wrong, we need data from L1, maybe just relay request and be done with it
die;
    d_relayDataToRequestor;
    l_popRequestQueue;
  }

action(printerr, desc="void")
{
	DPRINTF(RubySlicc, "Got FWD GETS when invalid\n");
        assert(false);
}

  transition(I, Fwd_GETS, I) {
printerr;
  }


  transition(E_S, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    d_relayDataToRequestor; blble
    d2_sendDataToL3;
    //todo InformL1
    l_popRequestQueue;
  }

  transition(E_M, {Fwd_GETS, Fwd_GET_INSTR}, S) {
	downgradeL1;
	d2_sendDataToL3;
    	l_popRequestQueue;
  }

  // Transitions from Modified
//  transition(M, {L1_GETX, L1_GET_INSTR}) {
//    sendExclusiveDataToL1
//	sendResponseToL1;
//l_popRequestQueue;
//  }

  //here we need M_I, because there will be two InvACKs
  transition(M, L2_Replacement, M_WB) {
    i_allocateTBE; // Why do we need TBE here?
    // Cannot be silent
    sendInvToL1;
//    g_issuePUTX;   // send data, but hold in case forwarded request TODO Data cannot be sent, value might have changed in L1
    ff_deallocateL2CacheBlock;
  }

  transition(M_I, WB_Data) {
	//hmm, L1 has evicted self and we caught it in M_I state, it is safe to do nothing, proper PUTX had to be issued earlier
	o_popIncomingResponseQueue;
  }

  transition(M_WB, {WB_Data_clean, WB_Data}, M_I) {
	//TODO update TBE
	t_sendWBAck;
	g_issuePUTXFromToL1q; //now send to L3
        o_popIncomingResponseQueue;
  }

  //TODO imho not, but we'll see, this would imply that L2 though that L1 was in M state, but it actually was in some other state. Wb_Ack only from L3, right?
//  transition(M_WB, WB_Ack, M_I) {
//	g_issuePUTXfromTBE;
//        o_popIncomingResponseQueue;
//  }

//TODO should not happen, L1 should send this only when L2 is in S_I state
transition(M_WB, Ack, I) {
//DPRINTF(RubySlicc, "I dunno, probably an error.\n");
        o_popIncomingResponseQueue;
}

//  transition(M_I, {WB_Ack, Ack}, M_II) { //used to be transition to I We need ACKs from both L1 and L3 TODO TODO TODO
//    s_deallocateTBE;
//    o_popIncomingResponseQueue;
//    kd_wakeUpDependents;
//  }

  transition(M_I, {WB_Ack, Ack}, I) { //used to be transition to I We need ACKs from both L1 and L3 TODO TODO TODO
    s_deallocateTBE; //kdyz toto odstranim, tak to nepada !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(M_II, {WB_Data, WB_Data_clean}, M_II) { //used to be transition to I We need ACKs from both L1 and L3 TODO TODO TODO
    a_relayResponseToL3;
    o_popIncomingResponseQueue;
  }

  transition(M, Inv, I) {
    f_sendDataToL3;
    l_popRequestQueue;
  }

  transition(M_I, Inv, SINK_WB_ACK) {
    ft_sendDataToL3_fromTBE;
    l_popRequestQueue;
  }

//TODO well, idiot, 'M' means that we cannot send data, only L1, can.
  transition(M, Fwd_GETX, I) {
    d_relayDataToRequestor;
    l_popRequestQueue;
  }

  transition(M, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    d_relayDataToRequestor;
    d2_sendDataToL3;
    l_popRequestQueue;
  }

//TODO something needs to be done, right?
//  transition(M_I, Fwd_GETX, SINK_WB_ACK) {
//    dt_sendDataToRequestor_fromTBE;
//    l_popRequestQueue;
//  }

//  transition(M_I, {Fwd_GETS, Fwd_GET_INSTR}, SINK_WB_ACK) {
//    dt_sendDataToRequestor_fromTBE;
//    d2t_sendDataToL3_fromTBE;
//    l_popRequestQueue;
//  }

  // Transitions from IS
  transition({IS, IS_I}, Inv, IS_I) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(IS, Data_all_Acks, S) {
    u_writeDataToL2CacheFromL3; //From what??
//    h_load_hit;
    s_deallocateTBE;
    sendResponseToL1;
    kd_wakeUpDependents;
    o_popIncomingResponseQueue;
  }

  transition(IS_I, Data_all_Acks, I) {
    u_writeDataToL2CacheFromL1; // From what???
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, DataS_fromL3, S) {
    u_writeDataToL2CacheFromL1; // From what???
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS_I, DataS_fromL3, I) { // ????????
    u_writeDataToL2CacheFromL1; // From what??
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // directory is blocked when sending exclusive data
  transition(IS_I, Data_Exclusive, E) {
    u_writeDataToL2CacheFromL1; // From what???
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, Data_Exclusive, E) {
    u_writeDataToL2CacheFromL1; // Frow what??
//    h_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // Transitions from IM
  transition({IM, SM}, Inv, IM) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(IM, Data, SM) {
    u_writeDataToL2CacheFromL1;
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(IM, Data_all_Acks, M) {
    u_writeDataToL2CacheFromL1;
//    hh_store_hit;
    s_deallocateTBE;
    sendResponseToL1;
    kd_wakeUpDependents;
    o_popIncomingResponseQueue;
  }

  // transitions from SM
  transition({SM, IM}, Ack) {
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(SM, Ack_all, M) {
//    hh_store_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(SINK_WB_ACK, Inv){
    fi_sendInvAck;
    l_popRequestQueue;
  }

 transition(S, WB_Ack) {
//   a_relayResponseToL1;
// An Extra state is needed here!!!!! TODO TODO TODO TODO, do not send ack to L1, it already in I state, because we've sent the ack before. Question is, whether we should change state here as well. Might be its not really needed. Who knows.
   o_popIncomingResponseQueue;
}

  transition(SINK_WB_ACK, WB_Ack, I) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }
//hotfix
  transition(SINK_WB_ACK, Ack, I) {
    o_popIncomingResponseQueue;
  }

//TODO i am not too sure about this.
  transition(SINK_WB_ACK, WB_Data) {
//weird, i should not be stalling responses!
	stall_incoming_l1_responses;
//	t_sendWBAck;
//    	o_popIncomingResponseQueue;
  }

// and this
  transition(SINK_WB_ACK, L1_PUTX) {
	t_sendWBAckFromReq;
	a_relayRequestToL3;
    	l_popRequestQueue;
  }

// and this too
  transition(I, WB_Ack)
{
    	o_popIncomingResponseQueue;
}

  // Transitions from Idle
  transition(I, L2_Replacement) { //just frees memory?
    // Nothing would be sufficient, right?
    ff_deallocateL2CacheBlock;
  }

}
