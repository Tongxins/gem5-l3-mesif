
/*
 * Copyright (c) 1999-2005 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * $Id: MSI_MOSI_CMP_directory-L3cache.sm 1.12 05/01/19 15:55:40-06:00 beckmann@s0-28.cs.wisc.edu $
 *
 */

machine(L3Cache, "MESIF Directory L3 Cache CMP")
 : CacheMemory * L3cacheMemory,
   int l3_request_latency = 2,  
   int l3_response_latency = 2,
   int to_l3_latency = 1
{
  // L3 BANK QUEUES
  // From local bank of L3 cache TO the network
  MessageBuffer DirRequestFromL3Cache, network="To", virtual_network="0", ordered="false", vnet_type="request";  // this L3 bank -> Memory
  MessageBuffer RequestFromL3Cache, network="To", virtual_network="0", ordered="false", vnet_type="request";  // this L3 bank -> a local L2
  MessageBuffer responseFromL3Cache, network="To", virtual_network="1", ordered="false", vnet_type="response";  // this L3 bank -> a local L2 || Memory

  // FROM the network to this local bank of L3 cache
  MessageBuffer unblockToL3Cache, network="From", virtual_network="2", ordered="false", vnet_type="unblock";  // a local L2 || Memory -> this L3 bank
  MessageBuffer RequestToL3Cache, network="From", virtual_network="0", ordered="false", vnet_type="request";  // a local L2 -> this L3 bank
  MessageBuffer responseToL3Cache, network="From", virtual_network="1", ordered="false", vnet_type="response";  // a local L2 || Memory -> this L3 bank

  // STATES
  state_declaration(State, desc="L3 Cache states", default="L3Cache_State_NP") {
    // Base states
    NP, AccessPermission:Invalid, desc="Not present in either cache";
    SS, AccessPermission:Read_Only, desc="L3 cache entry Shared, also present in one or more L2s";
    M, AccessPermission:Read_Write, desc="L3 cache entry Modified, not present in any L2s", format="!b";
    MT, AccessPermission:Maybe_Stale, desc="L3 cache entry Modified in a local L2, assume L3 copy stale", format="!b";

    // L3 replacement
    M_I, AccessPermission:Busy, desc="L3 cache replacing, have all acks, sent dirty data to memory, waiting for ACK from memory";
    MT_I, AccessPermission:Busy, desc="L3 cache replacing, getting data from exclusive";
    MCT_I, AccessPermission:Busy, desc="L3 cache replacing, clean in L3, getting data or ack from exclusive";
    I_I, AccessPermission:Busy, desc="L3 replacing clean data, need to inv sharers and then drop data";
    S_I, AccessPermission:Busy, desc="L3 replacing dirty data, collecting acks from L2s";

    // Transient States for fetching data from memory
    ISS, AccessPermission:Busy, desc="L3 idle, got single L2_GETS, issued memory fetch, have not seen response yet";
    IS, AccessPermission:Busy, desc="L3 idle, got L2_GET_INSTR or multiple L2_GETS, issued memory fetch, have not seen response yet";
    IM, AccessPermission:Busy, desc="L3 idle, got L2_GETX, issued memory fetch, have not seen response(s) yet";

    // Blocking states
    SS_MB, AccessPermission:Busy, desc="Blocked for L2_GETX from SS";
    MT_MB, AccessPermission:Busy, desc="Blocked for L2_GETX from MT"; //Blocks when already in L2, another L2 wants the value
    M_MB, AccessPermission:Busy, desc="Blocked for L2_GETX from M";

    MT_IIB, AccessPermission:Busy, desc="Blocked for L2_GETS from MT, waiting for unblock and data";
    MT_IB, AccessPermission:Busy, desc="Blocked for L2_GETS from MT, got unblock, waiting for data";
    MT_SB, AccessPermission:Busy, desc="Blocked for L2_GETS from MT, got data,  waiting for unblock";
 
  }

  // EVENTS
  enumeration(Event, desc="L3 Cache events") {
    // L3 events

    // events initiated by the local L2s
    L2_GET_INSTR,            desc="a L2I GET INSTR request for a block maped to us";
    L2_GETS,                 desc="a L2D GETS request for a block maped to us";
    L2_GETX,                 desc="a L2D GETX request for a block maped to us";
    L2_UPGRADE,                 desc="a L2D GETX request for a block maped to us";

    L2_PUTX,                 desc="L2 replacing data";
    L2_PUTX_old,             desc="L2 replacing data, but no longer sharer";

    Fwd_L2_GETX,             desc="L2 did not have data, so we supply";
    Fwd_L2_GETS,             desc="L2 did not have data, so we supply";
    Fwd_L2_GET_INSTR,             desc="L2 did not have data, so we supply";

    // events initiated by this L3
    L3_Replacement,     desc="L3 Replacement", format="!r";
    L3_Replacement_clean,     desc="L3 Replacement, but data is clean", format="!r";

    // events from memory controller
    Mem_Data,     desc="data from memory", format="!r";
    Mem_Ack,     desc="ack from memory", format="!r";

    // M->S data writeback
    WB_Data,  desc="data from L2";
    WB_Data_clean,  desc="clean data from L2";
    Ack,      desc="writeback ack";
    Ack_all,      desc="writeback ack";

    Unblock, desc="Unblock from L2 requestor";
    Unblock_Cancel, desc="Unblock from L2 requestor (FOR XACT MEMORY)";
    Exclusive_Unblock, desc="Unblock from L2 requestor";

    MEM_Inv, desc="Invalidation from directory";

  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry") {
    State CacheState,          desc="cache state";
    NetDest Sharers,               desc="tracks the L2 shares on-chip";
    MachineID Exclusive,          desc="Exclusive holder of block";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false", desc="data is dirty";
  }

  // TBE fields
  structure(TBE, desc="...") {
    Address Address,            desc="Physical address for this TBE";
    State TBEState,             desc="Transient state";
    DataBlock DataBlk,          desc="Buffer for the data block";
    bool Dirty, default="false", desc="Data is Dirty";

    NetDest L2_GetS_IDs,            desc="Set of the internal processors that want the block in shared state";
    MachineID L2_GetX_ID,          desc="ID of the L2 cache to forward the block to once we get a response";
    bool isPrefetch,            desc="Set if this was caused by a prefetch";

    int pendingAcks,            desc="number of pending acks for invalidates during writeback";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable L3_TBEs, template_hack="<L3Cache_TBE>";

  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);

  // inclusive cache, returns L3 entries only
  Entry getCacheEntry(Address addr), return_by_pointer="yes" {
    return static_cast(Entry, "pointer", L3cacheMemory[addr]);
  }

  std::string getCoherenceRequestTypeStr(CoherenceRequestType type) {
    return CoherenceRequestType_to_string(type);
  }

  bool isOneSharerLeft(Address addr, MachineID requestor, Entry cache_entry) {
    assert(is_valid(cache_entry));
    assert(cache_entry.Sharers.isElement(requestor));
    return (cache_entry.Sharers.count() == 1);
  }

  bool isSharer(Address addr, MachineID requestor, Entry cache_entry) {
    if (is_valid(cache_entry)) {
      return cache_entry.Sharers.isElement(requestor);
    } else {
      return false;
    }
  }

  void addSharer(Address addr, MachineID requestor, Entry cache_entry) {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "Adding sharer: machineID: %s, requestor: %s, address: %s\n",
            machineID, requestor, addr);
    assert(machineIDToMachineType(requestor) == MachineType:L2Cache || machineIDToMachineType(requestor) == MachineType:L3Cache);
    cache_entry.Sharers.add(requestor);
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {
    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:NP;
  }

  std::string getStateStr(TBE tbe, Entry cache_entry, Address addr) {
    return L3Cache_State_to_string(getState(tbe, cache_entry, addr));
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {

    // MUST CHANGE
    if (is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Address addr) {
    TBE tbe := L3_TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L3Cache_State_to_permission(tbe.TBEState));
      return L3Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L3Cache_State_to_permission(cache_entry.CacheState));
      return L3Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    return AccessPermission:NotPresent;
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    TBE tbe := L3_TBEs[addr];
    if(is_valid(tbe)) {
        return tbe.DataBlk;
    }

    return getCacheEntry(addr).DataBlk;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L3Cache_State_to_permission(state));
    }
  }

  Event L2Cache_request_type_to_event(CoherenceRequestType type, Address addr,
                                      MachineID requestor, Entry cache_entry) {
    if(type == CoherenceRequestType:GETS) {
      return Event:L2_GETS;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L2_GET_INSTR;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L2_GETX;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( is_valid(cache_entry) && cache_entry.Sharers.isElement(requestor) ) {
        return Event:L2_UPGRADE;
      } else {
        return Event:L2_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
      if (isSharer(addr, requestor, cache_entry)) {
        return Event:L2_PUTX;
      } else {
        return Event:L2_PUTX_old;
      }
    } else {
      DPRINTF(RubySlicc, "address: %s, Request Type: %s\n", addr, type);
      error("Invalid L2 forwarded request type");
    }
  }

  int getPendingAcks(TBE tbe) {
    assert(is_valid(tbe));
    return tbe.pendingAcks;
  }

  bool isDirty(Entry cache_entry) {
    assert(is_valid(cache_entry));
    return cache_entry.Dirty;
  }

  // ** OUT_PORTS **

  out_port(L2RequestIntraChipL3Network_out, RequestMsg, RequestFromL3Cache);
  out_port(DirRequestIntraChipL3Network_out, RequestMsg, DirRequestFromL3Cache);
  out_port(responseIntraChipL3Network_out, ResponseMsg, responseFromL3Cache);


  in_port(L2unblockNetwork_in, ResponseMsg, unblockToL3Cache, rank = 2) {
    if(L2unblockNetwork_in.isReady()) {
      peek(L2unblockNetwork_in,  ResponseMsg) {
        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L3_TBEs[in_msg.Address];
        DPRINTF(RubySlicc, "Addr: %s State: %s Sender: %s Type: %s Dest: %s\n",
                in_msg.Address, getState(tbe, cache_entry, in_msg.Address),
                in_msg.Sender, in_msg.Type, in_msg.Destination);

        assert(in_msg.Destination.isElement(machineID));
        if (in_msg.Type == CoherenceResponseType:EXCLUSIVE_UNBLOCK) {
          trigger(Event:Exclusive_Unblock, in_msg.Address, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceResponseType:UNBLOCK) {
          trigger(Event:Unblock, in_msg.Address, cache_entry, tbe);
        } else {
          error("unknown unblock message");
        }
      }
    }
  }

  // Response IntraChip L3 Network - response msg to this particular L3 bank
  in_port(responseIntraChipL3Network_in, ResponseMsg, responseToL3Cache, rank = 1) {
    if (responseIntraChipL3Network_in.isReady()) {
      peek(responseIntraChipL3Network_in, ResponseMsg) {
        // test wether it's from a local L2 or an off chip source
        assert(in_msg.Destination.isElement(machineID));
        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L3_TBEs[in_msg.Address];

	DPRINTF(RubySlicc, "Response to L3 in state %s: %s\n", getState(tbe, cache_entry, in_msg.Address), in_msg);

        if(machineIDToMachineType(in_msg.Sender) == MachineType:L2Cache) {
          if(in_msg.Type == CoherenceResponseType:DATA) {
            if (in_msg.Dirty) {
              trigger(Event:WB_Data, in_msg.Address, cache_entry, tbe);
            } else {
              trigger(Event:WB_Data_clean, in_msg.Address, cache_entry, tbe);
            }
          } else if (in_msg.Type == CoherenceResponseType:ACK) {
            if ((getPendingAcks(tbe) - in_msg.AckCount) == 0) {
              trigger(Event:Ack_all, in_msg.Address, cache_entry, tbe);
            } else {
              trigger(Event:Ack, in_msg.Address, cache_entry, tbe);
            }
          } else {
            error("unknown message type");
          }

        } else { // external message
          if(in_msg.Type == CoherenceResponseType:MEMORY_DATA) {
              // L3 now has data and all off-chip acks
              trigger(Event:Mem_Data, in_msg.Address, cache_entry, tbe);
          } else if(in_msg.Type == CoherenceResponseType:MEMORY_ACK) {
              // L3 now has data and all off-chip acks
              trigger(Event:Mem_Ack, in_msg.Address, cache_entry, tbe);
          } else if(in_msg.Type == CoherenceResponseType:INV) {
              // L3 now has data and all off-chip acks
              trigger(Event:MEM_Inv, in_msg.Address, cache_entry, tbe);
          } else {
            error("unknown message type");
          }
        }
      }
    }  // if not ready, do nothing
  }

  // L2 Request
  in_port(L2RequestIntraChipL3Network_in, RequestMsg, RequestToL3Cache, rank = 0) {
    if(L2RequestIntraChipL3Network_in.isReady()) {
      peek(L2RequestIntraChipL3Network_in,  RequestMsg) {
        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := L3_TBEs[in_msg.Address];

	DPRINTF(RubySlicc, "Request to L3 in state %s: %s\n", getState(tbe, cache_entry, in_msg.Address), in_msg);

        assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L2Cache);
        assert(in_msg.Destination.isElement(machineID));

        if (is_valid(cache_entry)) {
          // The L3 contains the block, so proceeded with handling the request
          trigger(L2Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                in_msg.Requestor, cache_entry),
                  in_msg.Address, cache_entry, tbe);
        } else {
          if (L3cacheMemory.cacheAvail(in_msg.Address)) {
            // L3 does't have the line, but we have space for it in the L3
            trigger(L2Cache_request_type_to_event(in_msg.Type, in_msg.Address,
                                                  in_msg.Requestor, cache_entry),
                    in_msg.Address, cache_entry, tbe);
          } else {
            // No room in the L3, so we need to make room before handling the request
            Entry L3cache_entry := getCacheEntry(L3cacheMemory.cacheProbe(in_msg.Address));
            if (isDirty(L3cache_entry)) {
              trigger(Event:L3_Replacement, L3cacheMemory.cacheProbe(in_msg.Address),
                      L3cache_entry, L3_TBEs[L3cacheMemory.cacheProbe(in_msg.Address)]);
            } else {
              trigger(Event:L3_Replacement_clean, L3cacheMemory.cacheProbe(in_msg.Address),
                      L3cache_entry, L3_TBEs[L3cacheMemory.cacheProbe(in_msg.Address)]);
            }
          }
        }
      }
    }
  }


  // ACTIONS

  action(a_issueFetchToMemory, "a", desc="fetch data from memory") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(DirRequestIntraChipL3Network_out, RequestMsg, latency=l3_request_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceRequestType:GETS;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(map_Address_to_Directory(address));
        out_msg.MessageSize := MessageSizeType:Control;
      }
    }
  }

  action(b_forwardRequestToExclusive, "b", desc="Forward request to the exclusive L2") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(L2RequestIntraChipL3Network_out, RequestMsg, latency=to_l3_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination.add(cache_entry.Exclusive);
        out_msg.MessageSize := MessageSizeType:Request_Control;
      }
    }
  }

  action(c_exclusiveReplacement, "c", desc="Send data to memory") {
    enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=l3_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:MEMORY_DATA;
      out_msg.Sender := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(c_exclusiveCleanReplacement, "cc", desc="Send ack to memory for clean replacement") {
    enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=l3_response_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:ACK;
      out_msg.Sender := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.MessageSize := MessageSizeType:Response_Control;
    }
  }

  action(ct_exclusiveReplacementFromTBE, "ct", desc="Send data to memory") {
    enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=l3_response_latency) {
      assert(is_valid(tbe));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:MEMORY_DATA;
      out_msg.Sender := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(d_sendDataToRequestor, "d", desc="Send data from cache to reqeustor") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=l3_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;

        out_msg.AckCount := 0 - cache_entry.Sharers.count();
        if (cache_entry.Sharers.isElement(in_msg.Requestor)) {
          out_msg.AckCount := out_msg.AckCount + 1;
        }
      }
    }
  }

  action(dd_sendExclusiveDataToRequestor, "dd", desc="Send data from cache to reqeustor") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=l3_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;

        out_msg.AckCount := 0 - cache_entry.Sharers.count();
        if (cache_entry.Sharers.isElement(in_msg.Requestor)) {
          out_msg.AckCount := out_msg.AckCount + 1;
        }
      }
    }
  }

  action(xy_assertNotSharer, "xy", desc="...") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
	assert(isSharer(in_msg.Address, in_msg.Requestor, cache_entry) == false);
    }
  }

  action(ds_sendSharedDataToRequestor, "ds", desc="Send data from cache to reqeustor") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=l3_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        out_msg.AckCount := 0;
      }
    }
  }

  action(e_sendDataToGetSRequestors, "e", desc="Send data from cache to all GetS IDs") {
    assert(is_valid(tbe));
    assert(tbe.L2_GetS_IDs.count() > 0);
    enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=to_l3_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.Sender := machineID;
      out_msg.Destination := tbe.L2_GetS_IDs;  // internal nodes
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(ex_sendExclusiveDataToGetSRequestors, "ex", desc="Send data from cache to all GetS IDs") {
    assert(is_valid(tbe));
    assert(tbe.L2_GetS_IDs.count() == 1);
    enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=to_l3_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
      out_msg.Sender := machineID;
      out_msg.Destination := tbe.L2_GetS_IDs;  // internal nodes
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(ee_sendDataToGetXRequestor, "ee", desc="Send data from cache to GetX ID") {
    enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=to_l3_latency) {
      assert(is_valid(tbe));
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.Sender := machineID;
      out_msg.Destination.add(tbe.L2_GetX_ID);
      DPRINTF(RubySlicc, "%s\n", out_msg.Destination);
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      DPRINTF(RubySlicc, "Address: %s, Destination: %s, DataBlock: %s\n",
              out_msg.Address, out_msg.Destination, out_msg.DataBlk);
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(f_sendInvToSharers, "f", desc="invalidate sharers for L3 replacement") {
    enqueue(L2RequestIntraChipL3Network_out, RequestMsg, latency=to_l3_latency) {
      DPRINTF(RubySlicc, "Sending INV to %s\n", cache_entry.Sharers);
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:INV;
      out_msg.Requestor := machineID;
      out_msg.Destination := cache_entry.Sharers;
      out_msg.MessageSize := MessageSizeType:Request_Control;
    }
  }

  action(fw_sendFwdInvToSharers, "fw", desc="invalidate sharers for request") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(L2RequestIntraChipL3Network_out, RequestMsg, latency=to_l3_latency) {
      DPRINTF(RubySlicc, "Sending INV to %s\n", cache_entry.Sharers);
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceRequestType:INV;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination := cache_entry.Sharers;
        out_msg.MessageSize := MessageSizeType:Request_Control;
      }
    }
  }

  action(fwm_sendFwdInvToSharersMinusRequestor, "fwm", desc="invalidate sharers for request, requestor is sharer") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(L2RequestIntraChipL3Network_out, RequestMsg, latency=to_l3_latency) {
      DPRINTF(RubySlicc, "Sending INV to %s\n", cache_entry.Sharers);
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceRequestType:INV;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination := cache_entry.Sharers;
        out_msg.Destination.remove(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Request_Control;
      }
    }
  }

  // OTHER ACTIONS
  action(i_allocateTBE, "i", desc="Allocate TBE for internal/external request(isPrefetch=0, number of invalidates=0)") {
    check_allocate(L3_TBEs);
    assert(is_valid(cache_entry));
    L3_TBEs.allocate(address);
    set_tbe(L3_TBEs[address]);
    tbe.L2_GetS_IDs.clear();
    tbe.DataBlk := cache_entry.DataBlk;
    tbe.Dirty := cache_entry.Dirty;
    tbe.pendingAcks := cache_entry.Sharers.count();
  }

  action(s_deallocateTBE, "s", desc="Deallocate external TBE") {
    L3_TBEs.deallocate(address);
    unset_tbe();
  }

  action(jj_popL2RequestQueue, "\j", desc="Pop incoming L2 request queue") {
    profileMsgDelay(0, L2RequestIntraChipL3Network_in.dequeue_getDelayCycles());
  }

  action(k_popUnblockQueue, "k", desc="Pop incoming unblock queue") {
    profileMsgDelay(0, L2unblockNetwork_in.dequeue_getDelayCycles());
  }

  action(o_popIncomingResponseQueue, "o", desc="Pop Incoming Response queue") {
    profileMsgDelay(1, responseIntraChipL3Network_in.dequeue_getDelayCycles());
  }

  action(m_writeDataToCache, "m", desc="Write data from response queue to cache") {
    peek(responseIntraChipL3Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(mr_writeDataToCacheFromRequest, "mr", desc="Write data from response queue to cache") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(q_updateAck, "q", desc="update pending ack count") {
    peek(responseIntraChipL3Network_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(qq_writeDataToTBE, "\qq", desc="Write data from response queue to TBE") {
    peek(responseIntraChipL3Network_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.DataBlk := in_msg.DataBlk;
      tbe.Dirty := in_msg.Dirty;
    }
  }

  action(z_stall, "z", desc="Stall") {
  }

  action(ss_recordGetSL2ID, "\s", desc="Record L2 GetS for load response") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(tbe));
      tbe.L2_GetS_IDs.add(in_msg.Requestor);
    }
  }

  action(xx_recordGetXL2ID, "\x", desc="Record L2 GetX for store response") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(tbe));
      tbe.L2_GetX_ID := in_msg.Requestor;
    }
  }

  action(set_setMRU, "\set", desc="set the MRU entry") {
    L3cacheMemory.setMRU(address);
  }

  action(qq_allocateL3CacheBlock, "\q", desc="Set L3 cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(L3cacheMemory.allocate(address, new Entry));
    }
  }

  action(rr_deallocateL3CacheBlock, "\r", desc="Deallocate L3 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    L3cacheMemory.deallocate(address);
    unset_cache_entry();
  }

  action(t_sendWBAck, "t", desc="Send writeback ACK") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=to_l3_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(t_sendWBAckResp, "tkk", desc="Send writeback ACK") {
    peek(responseIntraChipL3Network_in, ResponseMsg) {
      enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=to_l3_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Sender);
        out_msg.MessageSize := MessageSizeType:Response_Control;
  }
}
}
  action(ts_sendInvAckToUpgrader, "ts", desc="Send ACK to upgrader") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      enqueue(responseIntraChipL3Network_out, ResponseMsg, latency=to_l3_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        // upgrader doesn't get ack from itself, hence the + 1
        out_msg.AckCount := 0 - cache_entry.Sharers.count() + 1;
      }
    }
  }

  GenericRequestType convertToGenericType(CoherenceRequestType type) {
    if(type == CoherenceRequestType:GETS) {
      return GenericRequestType:GETS;
    } else if(type == CoherenceRequestType:GETX) {
      return GenericRequestType:GETX;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return GenericRequestType:GET_INSTR;
    } else if(type == CoherenceRequestType:UPGRADE) {
      return GenericRequestType:UPGRADE;
    } else {
      DPRINTF(RubySlicc, "%s\n", type);
      error("Invalid CoherenceRequestType\n");
    }
  }

  action(uu_profileMiss, "\u", desc="Profile the demand miss") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      L3cacheMemory.profileGenericRequest(convertToGenericType(in_msg.Type),
                                       in_msg.AccessMode, in_msg.Prefetch);
    }
  }

  action(ww_profileMissNoDir, "\w", desc="Profile this transition at the L3 because Dir won't see the request") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      // profile_request(in_msg.L2CacheStateStr, getStateStr(address), "NA", getCoherenceRequestTypeStr(in_msg.Type));
    }
  }

  action(nn_addSharer, "\n", desc="Add L2 sharer to list") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      addSharer(address, in_msg.Requestor, cache_entry);
      APPEND_TRANSITION_COMMENT( cache_entry.Sharers );
    }
  }

  action(nnu_addSharerFromUnblock, "\nu", desc="Add L2 sharer to list") {
    peek(L2unblockNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      addSharer(address, in_msg.Sender, cache_entry);
    }
  }

  action(kk_removeRequestSharer, "\k", desc="Remove L2 Request sharer from list") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.Sharers.remove(in_msg.Requestor);
    }
  }

  action(ll_clearSharers, "\l", desc="Remove all L2 sharers from list") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.Sharers.clear();
    }
  }

  action(mm_markExclusive, "\m", desc="set the exclusive owner") {
    peek(L2RequestIntraChipL3Network_in, RequestMsg) {
      assert(is_valid(cache_entry));
      cache_entry.Sharers.clear();
      cache_entry.Exclusive := in_msg.Requestor;
      addSharer(address, in_msg.Requestor, cache_entry);
    }
  }

  action(mmu_markExclusiveFromUnblock, "\mu", desc="set the exclusive owner") {
    peek(L2unblockNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.Sharers.clear();
      cache_entry.Exclusive := in_msg.Sender;
      addSharer(address, in_msg.Sender, cache_entry);
    }
  }

  action(zz_stallAndWaitL2RequestQueue, "zz", desc="recycle L2 request queue") {
    stall_and_wait(L2RequestIntraChipL3Network_in, address);
  }

  action(zn_recycleResponseNetwork, "zn", desc="recycle memory request") {
    responseIntraChipL3Network_in.recycle();
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  action(printBlockInfo, "omG", desc="dsaf") {
    DPRINTF(RubySlicc, "cache entry sharers: %s\n", cache_entry.Sharers);
  }

  //*****************************************************
  // TRANSITIONS
  //*****************************************************


  //===============================================
  // BASE STATE - I

  // Transitions from I (Idle)
  transition({NP, IS, ISS, IM, SS, M, M_I, I_I, S_I, M_MB, MT_IB, MT_SB}, L2_PUTX) {
    t_sendWBAck;    
    jj_popL2RequestQueue;
  }

  transition({NP, SS, M, MT, M_I, I_I, S_I, IS, ISS, IM, M_MB, MT_IB, MT_SB}, L2_PUTX_old) {
    t_sendWBAck;    
    jj_popL2RequestQueue;
  }

  transition({IM, IS, ISS, SS_MB, M_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, {L3_Replacement, L3_Replacement_clean}) {
    zz_stallAndWaitL2RequestQueue;
  }

  transition({IM, IS, ISS, SS_MB, M_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, MEM_Inv) {         
    zn_recycleResponseNetwork;
  }

  transition({S_I, M_I, MT_I}, MEM_Inv) {         
    o_popIncomingResponseQueue;
  }


  transition({SS_MB, M_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, {L2_GETS, L2_GET_INSTR, L2_GETX, L2_UPGRADE}) {
    printBlockInfo;
    zz_stallAndWaitL2RequestQueue;
  }


  transition(NP, L2_GETS,  ISS) {
    qq_allocateL3CacheBlock;
    ll_clearSharers;
    nn_addSharer;
    i_allocateTBE;
    ss_recordGetSL2ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL2RequestQueue;
  }

  transition(NP, L2_GET_INSTR, IS) {
    qq_allocateL3CacheBlock;
    ll_clearSharers;
    nn_addSharer;
    i_allocateTBE;
    ss_recordGetSL2ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL2RequestQueue;
  }

  transition(NP, L2_GETX, IM) {
    qq_allocateL3CacheBlock;
    ll_clearSharers;
    // nn_addSharer;
    i_allocateTBE;
    xx_recordGetXL2ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL2RequestQueue;
  }


  // transitions from IS/IM

  transition(ISS, Mem_Data, MT_MB) {
    m_writeDataToCache;
    ex_sendExclusiveDataToGetSRequestors;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition(IS, Mem_Data, SS) {
    m_writeDataToCache;
    e_sendDataToGetSRequestors;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IM, Mem_Data, MT_MB) {
    m_writeDataToCache;
    ee_sendDataToGetXRequestor;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition({IS, ISS}, {L2_GETS, L2_GET_INSTR}, IS) {
    nn_addSharer;
    ss_recordGetSL2ID;
    uu_profileMiss;
    jj_popL2RequestQueue;
  }

  transition({IS, ISS}, L2_GETX) {
    zz_stallAndWaitL2RequestQueue;
  }

  transition(IM, {L2_GETX, L2_GETS, L2_GET_INSTR}) {
    zz_stallAndWaitL2RequestQueue;
  }

  // transitions from SS
  transition(SS, {L2_GETS, L2_GET_INSTR}) {
    ds_sendSharedDataToRequestor;
    nn_addSharer;
    set_setMRU;
    jj_popL2RequestQueue;
  }


  transition(SS, L2_GETX, SS_MB) {
    d_sendDataToRequestor;
    // fw_sendFwdInvToSharers;
    fwm_sendFwdInvToSharersMinusRequestor;
    set_setMRU;
    jj_popL2RequestQueue;
  }

  transition(SS, L2_UPGRADE, SS_MB) {
    fwm_sendFwdInvToSharersMinusRequestor;
    ts_sendInvAckToUpgrader;
    set_setMRU;
    jj_popL2RequestQueue;
  }

  transition(SS, L3_Replacement_clean, I_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL3CacheBlock;
  }

  transition(SS, {L3_Replacement, MEM_Inv}, S_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL3CacheBlock;
  }


  transition(M, L2_GETX, MT_MB) {
    d_sendDataToRequestor;
    set_setMRU;
    jj_popL2RequestQueue;
  }

  transition(M, L2_GET_INSTR, SS) {
    d_sendDataToRequestor;
    nn_addSharer;
    set_setMRU;
    jj_popL2RequestQueue;
  }

  transition(M, L2_GETS, MT_MB) {
    dd_sendExclusiveDataToRequestor;
    set_setMRU;
    jj_popL2RequestQueue;
  }

  transition(M, {L3_Replacement, MEM_Inv}, M_I) {
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL3CacheBlock;
  }

  transition(M, L3_Replacement_clean, M_I) {
    i_allocateTBE;
    c_exclusiveCleanReplacement;
    rr_deallocateL3CacheBlock;
  }


  // transitions from MT

  transition(MT, L2_GETX, MT_MB) {
    xy_assertNotSharer;
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL2RequestQueue;
  }


  transition(MT, {L2_GETS, L2_GET_INSTR}, MT_IIB) {
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL2RequestQueue;
  }

  transition(MT, {L3_Replacement, MEM_Inv}, MT_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL3CacheBlock;
  }

  transition(MT, L3_Replacement_clean, MCT_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL3CacheBlock;
  }

  transition(MT, L2_PUTX, M) { //L2 has evicted itself
    ll_clearSharers;
    mr_writeDataToCacheFromRequest;
    t_sendWBAck;
    jj_popL2RequestQueue;
  }


  // transitions from blocking states
  transition(SS_MB, Unblock_Cancel, SS) {
    k_popUnblockQueue;
    kd_wakeUpDependents;
  }

  transition(MT_MB, Unblock_Cancel, MT) {
    k_popUnblockQueue;
    kd_wakeUpDependents;
  }

  transition(MT_IB, Unblock_Cancel, MT) {
    k_popUnblockQueue;
    kd_wakeUpDependents;
  }

  transition(SS_MB, Exclusive_Unblock, MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    k_popUnblockQueue;
    kd_wakeUpDependents;
  }

  transition({M_MB, MT_MB}, Exclusive_Unblock, MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    k_popUnblockQueue;
    kd_wakeUpDependents;
  }
  
  transition(MT_IIB, {L2_PUTX, L2_PUTX_old}){
    zz_stallAndWaitL2RequestQueue;
  }

  transition(MT_IIB, Unblock, MT_IB) {
    nnu_addSharerFromUnblock;
    k_popUnblockQueue;
  }

  transition(MT_IIB, {WB_Data, WB_Data_clean}, MT_SB) {
    m_writeDataToCache;
    o_popIncomingResponseQueue;
  }

  transition(MT_IB, {WB_Data, WB_Data_clean}, SS) {
    m_writeDataToCache;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(MT_SB, Unblock, SS) {
    nnu_addSharerFromUnblock;
    k_popUnblockQueue;
    kd_wakeUpDependents;
  }

  // writeback states
  transition({I_I, S_I, MT_I, MCT_I, M_I}, {L2_GETX, L2_UPGRADE, L2_GETS, L2_GET_INSTR}) {
    zz_stallAndWaitL2RequestQueue;
  }

  transition(I_I, Ack) {
    q_updateAck;
    o_popIncomingResponseQueue;
  }

  transition(I_I, Ack_all, M_I) {
    c_exclusiveCleanReplacement;
    o_popIncomingResponseQueue;
  }

  transition({MT_I, MCT_I}, WB_Data, M_I) {
    qq_writeDataToTBE;
    ct_exclusiveReplacementFromTBE;
    t_sendWBAckResp; // TODO added by me, I guess it should be like that, right?
    o_popIncomingResponseQueue;
  }

  transition(MCT_I, {WB_Data_clean, Ack_all}, M_I) {
    c_exclusiveCleanReplacement;   
    o_popIncomingResponseQueue;
  }

  transition(MCT_I,  {L2_PUTX, L2_PUTX_old}){
    zz_stallAndWaitL2RequestQueue; 
  }
  
  // L2 never changed Dirty data
  transition(MT_I, Ack_all, M_I) {
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
  }

  transition(MT_I, {L2_PUTX, L2_PUTX_old}){
    zz_stallAndWaitL2RequestQueue; 
  }

  // possible race between unblock and immediate replacement
  transition({MT_MB,SS_MB}, {L2_PUTX, L2_PUTX_old}) {
    zz_stallAndWaitL2RequestQueue;
  }

  transition(MT_I, WB_Data_clean, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(S_I, Ack) {
    q_updateAck;
    o_popIncomingResponseQueue;
  }

  transition(S_I, Ack_all, M_I) {
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
  }

  transition(M_I, Mem_Ack, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }


  /////////////////////////////////////stuff below added by me

  transition(M_I, Ack_all) {
    o_popIncomingResponseQueue; // TODO added by me, who knows
  }

  transition({MT, M}, Exclusive_Unblock) { // TODO this was added by me. I guess this happens when L2 has the value and sends it to L1. I could filter these messages at L2, but oh well.
    k_popUnblockQueue;
  }
  
  transition(NP, Ack) {
    o_popIncomingResponseQueue;
  }
}
